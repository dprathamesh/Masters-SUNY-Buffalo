{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyv_QR2r33yw"
      },
      "source": [
        "# **Phase 4**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FFR1EgW36q0"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner seaborn xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bgFEO6n7hkN"
      },
      "outputs": [],
      "source": [
        "#some of the imports may be repeated\n",
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovMuTiIe7pk6"
      },
      "outputs": [],
      "source": [
        "#some of the code blocks may be repeated\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/path/to/csv/cleaned_dataset.csv')\n",
        "\n",
        "df = df.sample(frac=0.25, random_state=42)\n",
        "df = df.sample(frac=0.25, random_state=42)\n",
        "\n",
        "# If you need to sample the data\n",
        "# df = df.sample(frac=0.25, random_state=42)\n",
        "# df = df.sample(frac=0.25, random_state=42)\n",
        "\n",
        "#Feature Encoding\n",
        "df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'])\n",
        "df['DayOfWeek'] = df['CRASH DATE'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "df['CRASH TIME'] = pd.to_datetime(df['CRASH TIME'], format='%H:%M')\n",
        "df['HourOfDay'] = df['CRASH TIME'].dt.hour  # Extract hour (0-23)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['BOROUGH'], drop_first=True)  # One-hot encoding (give values 0 or 1 to the categorical values) for boroughs\n",
        "df['ZIP CODE'] = pd.to_numeric(df['ZIP CODE'], errors='coerce')  # zip to numeric\n",
        "\n",
        "\n",
        "\n",
        "for col in ['CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3']:\n",
        "    df[col].fillna(\"Unknown\", inplace=True)  # missing values marked as \"Unknown\"\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "\n",
        "\n",
        "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "df.replace('', float('nan'), inplace=True)\n",
        "\n",
        "df['ZIP CODE'] = pd.to_numeric(df['ZIP CODE'], errors='coerce')\n",
        "\n",
        "features_clustering = [\n",
        "    'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
        "    'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
        "    'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
        "    'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED',\n",
        "    'DayOfWeek', 'HourOfDay',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 3'\n",
        "]\n",
        "\n",
        "#ensure df_filtered has only these 13 columns\n",
        "df_filtered = df[features_clustering].dropna()\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLqI0yx37xkO"
      },
      "outputs": [],
      "source": [
        "df['SEVERE_ACCIDENT'] = (df['NUMBER OF PERSONS INJURED'] > 0) | (df['NUMBER OF PERSONS KILLED'] > 0)\n",
        "df['SEVERE_ACCIDENT'] = df['SEVERE_ACCIDENT'].astype(int)\n",
        "\n",
        "df['BOROUGH'] = df['BOROUGH_BROOKLYN'] | df['BOROUGH_MANHATTAN'] | df['BOROUGH_QUEENS'] | df['BOROUGH_STATEN ISLAND']\n",
        "\n",
        "df['CRASH TIME'] = pd.to_datetime(df['CRASH TIME'])\n",
        "df['CRASH HOUR'] = df['CRASH TIME'].dt.hour\n",
        "\n",
        "features = [\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1', 'BOROUGH', 'CRASH HOUR',\n",
        "    'VEHICLE TYPE CODE 1', 'LATITUDE', 'LONGITUDE'\n",
        "]\n",
        "X = df[features]\n",
        "y = df['SEVERE_ACCIDENT']\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_features = ['CRASH HOUR', 'LATITUDE', 'LONGITUDE']\n",
        "categorical_features = ['CONTRIBUTING FACTOR VEHICLE 1', 'BOROUGH', 'VEHICLE TYPE CODE 1']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAexzrNrlB3X"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# 1.  Load data & inspect columns #this would help KMeans\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "RAW_PATH = \"/path/to/csv/cleaned_dataset.csv\"\n",
        "df = pd.read_csv(RAW_PATH)\n",
        "\n",
        "df = df.sample(frac=0.25, random_state=42)\n",
        "df = df.sample(frac=0.25, random_state=42)\n",
        "\n",
        "print(\"🗒️ Columns in df:\")\n",
        "for col in df.columns:\n",
        "    print(\"  •\", col)\n",
        "\n",
        "# If you have 'HourOfDay' instead of 'CRASH HOUR', rename it\n",
        "if 'HourOfDay' in df.columns and 'CRASH HOUR' not in df.columns:\n",
        "    df['CRASH HOUR'] = df['HourOfDay']\n",
        "    print(\"✅ Renamed 'HourOfDay' → 'CRASH HOUR'\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 2.  Feature engineering & target\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'])\n",
        "df['DayOfWeek'] = df['CRASH DATE'].dt.dayofweek\n",
        "\n",
        "df['CRASH TIME'] = pd.to_datetime(df['CRASH TIME'], format='%H:%M', errors='coerce')\n",
        "df['HourOfDay'] = df['CRASH TIME'].dt.hour  # backup\n",
        "df['CRASH HOUR'] = df['CRASH TIME'].dt.hour  # ensure this exists\n",
        "\n",
        "df['ZIP CODE'] = pd.to_numeric(df['ZIP CODE'], errors='coerce')\n",
        "\n",
        "# Fill & label‐encode contributing factors\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "for col in ['CONTRIBUTING FACTOR VEHICLE 1',\n",
        "            'CONTRIBUTING FACTOR VEHICLE 2',\n",
        "            'CONTRIBUTING FACTOR VEHICLE 3']:\n",
        "    df[col].fillna(\"Unknown\", inplace=True)\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Create binary target\n",
        "df['SEVERE_ACCIDENT'] = (\n",
        "    (df['NUMBER OF PERSONS INJURED'] > 0) |\n",
        "    (df['NUMBER OF PERSONS KILLED'] > 0)\n",
        ").astype(int)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 3.  Persist standalone borough encoder #one hot encoding for standardization\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import joblib\n",
        "\n",
        "borough_ohe = OneHotEncoder(\n",
        "    sparse_output=False,\n",
        "    handle_unknown=\"ignore\"\n",
        ")\n",
        "borough_ohe.fit(df[['BOROUGH']])\n",
        "joblib.dump(borough_ohe, 'borough_encoder.pkl')\n",
        "print(\"✅ Saved borough_encoder.pkl with:\", borough_ohe.categories_[0])\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 4.  Build & save full preprocessor\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_feats     = ['CRASH HOUR', 'LATITUDE', 'LONGITUDE']\n",
        "categorical_feats = ['CONTRIBUTING FACTOR VEHICLE 1', 'BOROUGH', 'VEHICLE TYPE CODE 1']\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    (\"num\", StandardScaler(), numeric_feats),\n",
        "    (\"cat\", OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\"),\n",
        "     categorical_feats),\n",
        "])\n",
        "\n",
        "# Fit on raw X (with BOROUGH as string)\n",
        "preprocessor.fit(df[numeric_feats + categorical_feats])\n",
        "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
        "print(\"✅ Saved preprocessor.pkl (BOROUGH categories: \",\n",
        "      preprocessor.named_transformers_['cat'].categories_[1], \")\")\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 5.  Transform features and split for modeling #this is how we prevent overfitting\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[numeric_feats + categorical_feats]\n",
        "y = df['SEVERE_ACCIDENT']\n",
        "\n",
        "X_proc = preprocessor.transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_proc, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"✅ Data ready: X_train.shape =\", X_train.shape, \", y_train.mean() =\", y_train.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo59LzX871Uo"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# XGBoost\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Impute any missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed  = imputer.transform(X_test)\n",
        "\n",
        "#Set up grid search\n",
        "param_grid_xgb = {\n",
        "    'n_estimators':    [50, 100, 200],\n",
        "    'learning_rate':   [0.01, 0.1, 0.2],\n",
        "    'max_depth':       [3, 5, 7],\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "grid_xgb  = GridSearchCV(\n",
        "    xgb_model,\n",
        "    param_grid_xgb,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "#Fit & select best\n",
        "grid_xgb.fit(X_train_imputed, y_train)\n",
        "best_xgb = grid_xgb.best_estimator_\n",
        "xgb_model = best_xgb  # ensure we save the fitted model\n",
        "\n",
        "#Predict & evaluate\n",
        "y_pred_xgb    = best_xgb.predict(X_test_imputed)\n",
        "confusion_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"XGBoost Best Parameters:\", grid_xgb.best_params_)\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"XGBoost F1 Score:\", f1_score(y_test, y_pred_xgb))\n",
        "\n",
        "#Plotting confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_xgb, annot=True, fmt='d', cmap='Purples')\n",
        "plt.title('XGBoost Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWeENxiq752Z"
      },
      "outputs": [],
      "source": [
        "#KMEANS\n",
        "inertia = []\n",
        "K_range = range(2, 10)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(df_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow graph\n",
        "plt.plot(K_range, inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.show()\n",
        "\n",
        "# Reduce dimensions to 2D for visualization\n",
        "#pca = PCA(n_components=2)\n",
        "#df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "k_range = range(3, 10)\n",
        "scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(df_scaled)\n",
        "    score = silhouette_score(df_scaled, labels)\n",
        "    scores.append({k: score})\n",
        "    print(f\"Silhouette score for k={k}: {score:.3f}\")\n",
        "\n",
        "optimal_k = max(scores, key=lambda x: list(x.values())[0])\n",
        "optimal_k = list(optimal_k.keys())[0]\n",
        "print(f\"Optimal k: {optimal_k}\")\n",
        "\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "kmeans.fit(df_scaled)\n",
        "\n",
        "# Reduce dimensions to 2D for visualization\n",
        "pca = PCA(n_components=2)\n",
        "df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Add cluster labels to PCA dataframe\n",
        "df_plot = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n",
        "df_plot['Cluster'] = kmeans.labels_\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "for cluster in range(optimal_k):\n",
        "    cluster_data = df_plot[df_plot['Cluster'] == cluster]\n",
        "    plt.scatter(cluster_data['PC1'], cluster_data['PC2'], label=f'Cluster {cluster}')\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Clusters Visualization')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pUUYijx78zZ"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet streamlit pyngrok joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oO0_MhX98AGC"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from google.colab import files\n",
        "# after training...\n",
        "# joblib.dump(preprocessor,   \"preprocessor.pkl\")\n",
        "joblib.dump(xgb_model,      \"xgb_model.pkl\")\n",
        "# joblib.dump(kmeans,      \"kmeans.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpRUsIW5lyoC"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# (Re)building and save a standalone borough encoder #this would be effective for a single model\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import joblib\n",
        "import streamlit as st\n",
        "\n",
        "# path to your raw DataFrame CSV\n",
        "DATA_PATH = '/path/to/csv/cleaned_dataset.csv'\n",
        "\n",
        "# Load raw data and fit an OHE on the BOROUGH column\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "borough_ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "borough_ohe.fit(df[[\"BOROUGH\"]])\n",
        "# Persist it\n",
        "joblib.dump(borough_ohe, \"borough_encoder.pkl\")\n",
        "print(\"Created borough_encoder.pkl with:\", borough_ohe.categories_[0])\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 1.  Debug: inspect your main preprocessor + new borough encoder\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# Force Streamlit to clear any cached models\n",
        "st.cache_data.clear()\n",
        "\n",
        "# Reload artifacts\n",
        "preprocessor = joblib.load(\"preprocessor.pkl\")\n",
        "xgb_model    = joblib.load(\"xgb_model.pkl\")\n",
        "\n",
        "# Reconstruct feature lists\n",
        "feature_names = list(preprocessor.feature_names_in_)\n",
        "def flatten(keys):\n",
        "    if isinstance(keys, (list, tuple)):\n",
        "        out = []\n",
        "        for k in keys:\n",
        "            out.extend(flatten(k))\n",
        "        return out\n",
        "    return [keys]\n",
        "\n",
        "num_keys = flatten(preprocessor.transformers_[0][2])\n",
        "cat_keys = flatten(preprocessor.transformers_[1][2])\n",
        "numeric_names     = [feature_names[i] if isinstance(i,int) else i for i in num_keys]\n",
        "categorical_names = [feature_names[i] if isinstance(i,int) else i for i in cat_keys]\n",
        "\n",
        "# Your pipeline’s OHE\n",
        "ohe = preprocessor.named_transformers_['cat']\n",
        "\n",
        "# Standalone borough encoder you just built\n",
        "borough_enc = joblib.load(\"borough_encoder.pkl\")\n",
        "\n",
        "# Print to console\n",
        "print(\"🔹 Numeric features:\", numeric_names)\n",
        "print(\"🔹 Categorical features:\", categorical_names)\n",
        "print(\"🔹 Pipeline OHE categories per feature:\")\n",
        "for name, cats in zip(categorical_names, ohe.categories_):\n",
        "    print(f\"   • {name}: {cats}\")\n",
        "print(\"🔹 Standalone borough_encoder categories:\", borough_enc.categories_[0])\n",
        "\n",
        "# Minimal Streamlit output to trigger a run\n",
        "st.write(\"✅ Debug info printed to your console. Check your terminal for the full list.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boy7rEM0l6al"
      },
      "outputs": [],
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# PREPARING CLUSTERING DATA IN‐MEMORY (no extra files)\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1) Load & downsample exactly as before\n",
        "df = pd.read_csv(\"/path/to/csv/cleaned_dataset.csv\")\n",
        "df = df.sample(frac=0.25, random_state=42).sample(frac=0.25, random_state=42)\n",
        "\n",
        "# 2) Feature engineering\n",
        "df['CRASH TIME'] = pd.to_datetime(df['CRASH TIME'], format='%H:%M', errors='coerce')\n",
        "df['CRASH HOUR'] = df['CRASH TIME'].dt.hour\n",
        "\n",
        "df['BOROUGH'].fillna(\"Unknown\", inplace=True)\n",
        "df['LATITUDE'].fillna(df['LATITUDE'].mean(), inplace=True)\n",
        "df['LONGITUDE'].fillna(df['LONGITUDE'].mean(), inplace=True)\n",
        "\n",
        "for col in [\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 2',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 3'\n",
        "]:\n",
        "    df[col].fillna(\"Unknown\", inplace=True)\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# 3) Build raw feature matrix\n",
        "features = [\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1',\n",
        "    'BOROUGH',\n",
        "    'CRASH HOUR',\n",
        "    'VEHICLE TYPE CODE 1',\n",
        "    'LATITUDE',\n",
        "    'LONGITUDE'\n",
        "]\n",
        "X_raw = df[features]\n",
        "\n",
        "# 4) Transform via your saved preprocessor\n",
        "preprocessor = joblib.load(\"preprocessor.pkl\")\n",
        "X_proc       = preprocessor.transform(X_raw)\n",
        "\n",
        "# 5) Drop any residual NaNs\n",
        "mask    = ~np.isnan(X_proc).any(axis=1)\n",
        "X_clean = X_proc[mask]\n",
        "\n",
        "# 6) Run PCA → 2 components\n",
        "pca   = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_clean)\n",
        "\n",
        "# 7) Keep X_pca in the notebook’s namespace for Streamlit to pick up\n",
        "print(\"✅ Prepared X_pca in‐memory (shape\", X_pca.shape, \")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBW4bMd88DGI"
      },
      "outputs": [],
      "source": [
        "app_code = \"\"\"\\\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 1) Load prediction artifacts & KMeans\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "@st.cache_data\n",
        "def load_models():\n",
        "    preprocessor    = joblib.load(\"preprocessor.pkl\")     # for severity\n",
        "    xgb_model       = joblib.load(\"xgb_model.pkl\")\n",
        "    kmeans          = joblib.load(\"kmeans.pkl\")      # trained on df_scaled\n",
        "    borough_encoder = joblib.load(\"borough_encoder.pkl\")\n",
        "    return preprocessor, xgb_model, kmeans, borough_encoder\n",
        "\n",
        "preprocessor, xgb_model, kmeans, borough_encoder = load_models()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 2) Re-create your original df_scaled & PCA coordinates\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "@st.cache_data\n",
        "def build_clustering_data():\n",
        "    df = pd.read_csv(\"/path/to/csv/cleaned_dataset.csv\")\n",
        "\n",
        "    # Downsample exactly as before\n",
        "    df = df.sample(frac=0.25, random_state=42).sample(frac=0.25, random_state=42)\n",
        "\n",
        "    # Create target and drop unused columns\n",
        "    df['CRASH TIME'] = pd.to_datetime(df['CRASH TIME'], format='%H:%M', errors='coerce')\n",
        "    df['CRASH HOUR'] = df['CRASH TIME'].dt.hour\n",
        "\n",
        "    # Fill and encode factors\n",
        "    for col in ['CONTRIBUTING FACTOR VEHICLE 1',\n",
        "                'CONTRIBUTING FACTOR VEHICLE 2',\n",
        "                'CONTRIBUTING FACTOR VEHICLE 3']:\n",
        "        df[col] = df[col].fillna(\"Unknown\")\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "    # Define exactly the clustering features\n",
        "    features_clustering = [\n",
        "        'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
        "        'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
        "        'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
        "        'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED',\n",
        "        'DayOfWeek', 'HourOfDay',\n",
        "        'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2',\n",
        "        'CONTRIBUTING FACTOR VEHICLE 3'\n",
        "    ]\n",
        "\n",
        "    # Make sure these exist\n",
        "    df['DayOfWeek'] = pd.to_datetime(df['CRASH DATE']).dt.dayofweek\n",
        "    df['HourOfDay'] = df['CRASH TIME'].dt.hour\n",
        "\n",
        "    df_filtered = df[features_clustering].dropna()\n",
        "\n",
        "    # Scale just like you did in training\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(df_filtered)\n",
        "\n",
        "    # PCA down to 2D\n",
        "    pca = PCA(n_components=2)\n",
        "    df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "    return df_scaled, df_pca\n",
        "\n",
        "X_scaled, X_pca = build_clustering_data()\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 3) Build multi‐view UI\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "st.title(\"🚦 NYC Crash Analyzer\")\n",
        "tab1, tab2 = st.tabs([\"Severity Prediction\", \"Cluster Explorer\"])\n",
        "\n",
        "# Metadata for severity prediction\n",
        "feature_names     = list(preprocessor.feature_names_in_)\n",
        "cat_transformer   = preprocessor.named_transformers_['cat']\n",
        "categorical_names = list(cat_transformer.feature_names_in_)\n",
        "borough_list      = list(borough_encoder.categories_[0])\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "with tab1:\n",
        "    st.header(\"🔍 Accident Severity Predictor\")\n",
        "    st.sidebar.header(\"Input Features\")\n",
        "\n",
        "    inputs = {}\n",
        "    for feat in feature_names:\n",
        "        if feat == \"BOROUGH\":\n",
        "            inputs[feat] = st.sidebar.selectbox(\"Borough\", borough_list, key=f\"cat_{feat}\")\n",
        "        elif feat in categorical_names:\n",
        "            idx  = categorical_names.index(feat)\n",
        "            opts = cat_transformer.categories_[idx]\n",
        "            inputs[feat] = st.sidebar.selectbox(feat, opts, key=f\"cat_{feat}\")\n",
        "        else:\n",
        "            if feat == \"CRASH HOUR\":\n",
        "                inputs[feat] = st.sidebar.slider(feat, 0, 23, 12, key=f\"num_{feat}\")\n",
        "            else:\n",
        "                inputs[feat] = st.sidebar.number_input(feat, value=0.0, key=f\"num_{feat}\")\n",
        "\n",
        "    X_df   = pd.DataFrame([inputs], columns=feature_names)\n",
        "    X_test = preprocessor.transform(X_df)\n",
        "    pred   = xgb_model.predict(X_test)[0]\n",
        "    proba  = xgb_model.predict_proba(X_test)[0]\n",
        "\n",
        "    st.markdown(f\"**Predicted severity:** `{pred}`\")\n",
        "    st.dataframe(\n",
        "        pd.DataFrame([proba], columns=[f\"Class {c}\" for c in xgb_model.classes_]),\n",
        "        use_container_width=True\n",
        "    )\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "with tab2:\n",
        "    st.header(\"📊 Cluster Explorer\")\n",
        "\n",
        "    x_axis = st.selectbox(\"X-axis\", [\"PC1\", \"PC2\"], index=0)\n",
        "    y_axis = st.selectbox(\"Y-axis\", [\"PC1\", \"PC2\"], index=1)\n",
        "    axis_idx = {\"PC1\": 0, \"PC2\": 1}\n",
        "    xi, yi   = axis_idx[x_axis], axis_idx[y_axis]\n",
        "\n",
        "    # Correctly predict on the 13-d scaled data\n",
        "    labels = kmeans.predict(X_scaled)\n",
        "\n",
        "    plot_df = pd.DataFrame({\n",
        "        x_axis: X_pca[:, xi],\n",
        "        y_axis: X_pca[:, yi],\n",
        "        \"Cluster\": labels.astype(str)\n",
        "    })\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    for cl in sorted(plot_df[\"Cluster\"].unique()):\n",
        "        subset = plot_df[plot_df[\"Cluster\"] == cl]\n",
        "        ax.scatter(subset[x_axis], subset[y_axis], label=f\"Cluster {cl}\", alpha=0.6)\n",
        "    ax.set_xlabel(x_axis)\n",
        "    ax.set_ylabel(y_axis)\n",
        "    ax.legend()\n",
        "    st.pyplot(fig)\n",
        "\"\"\"\n",
        "\n",
        "# write out the file\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "print(\"app.py has been updated with borough dropdowns and improved UI\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNQ18Ojr8IkK"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken 2veWgSMKSZUXoq3cDHk6gj7KPy2_2Soz335NRQEtrGw9tLcwC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsXQnhKG8KoC"
      },
      "outputs": [],
      "source": [
        "#Exposingg Streamlit’s port (8501) via ngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate any previous tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Open a new tunnel\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "print(\"→ Streamlit will be available at:\", public_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSbM8KNW8OCM"
      },
      "outputs": [],
      "source": [
        "!nohup streamlit run app.py --server.port 8501 > streamlit.log 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76-C7RTrmGgE"
      },
      "outputs": [],
      "source": [
        "!pkill ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OpSidtpmHu5"
      },
      "outputs": [],
      "source": [
        "!streamlit cache clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMLtsAyY8Qby"
      },
      "outputs": [],
      "source": [
        "import joblib, inspect, xgboost as xgb\n",
        "obj = joblib.load(\"xgb_model.pkl\")\n",
        "print(type(obj))\n",
        "if isinstance(obj, xgb.XGBClassifier):\n",
        "    print(\"Booster attached:\", hasattr(obj, \"_Booster\"))\n",
        "    print(\"n_features_in_  :\", getattr(obj, \"n_features_in_\", None))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
