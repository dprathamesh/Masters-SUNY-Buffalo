{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 2**"
      ],
      "metadata": {
        "id": "HcciLnwSw9VG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvZMqf35xEJ5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner as kt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsBJrxz9xEJ6"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('path to file/cleaned_dataset.csv')\n",
        "\n",
        "# If you need to sample the data\n",
        "# df = df.sample(frac=0.25, random_state=42)\n",
        "# df = df.sample(frac=0.25, random_state=42)\n",
        "\n",
        "#Feature Encoding\n",
        "df['CRASH DATE'] = pd.to_datetime(df['CRASH DATE'])\n",
        "df['DayOfWeek'] = df['CRASH DATE'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "df['CRASH TIME'] = pd.to_datetime(df['CRASH TIME'], format='%H:%M')\n",
        "df['HourOfDay'] = df['CRASH TIME'].dt.hour  # Extract hour (0-23)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['BOROUGH'], drop_first=True)  # One-hot encoding (give values 0 or 1 to the categorical values) for boroughs\n",
        "df['ZIP CODE'] = pd.to_numeric(df['ZIP CODE'], errors='coerce')  # zip to numeric\n",
        "\n",
        "\n",
        "\n",
        "for col in ['CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3']:\n",
        "    df[col].fillna(\"Unknown\", inplace=True)  # missing values marked as \"Unknown\"\n",
        "    df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "\n",
        "\n",
        "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "df.replace('', float('nan'), inplace=True)\n",
        "\n",
        "df['ZIP CODE'] = pd.to_numeric(df['ZIP CODE'], errors='coerce')\n",
        "\n",
        "features_clustering = [\n",
        "    'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
        "    'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
        "    'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
        "    'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED',\n",
        "    'DayOfWeek', 'HourOfDay',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 3'\n",
        "]\n",
        "\n",
        "# Ensure df_filtered has only these 13 columns\n",
        "df_filtered = df[features_clustering].dropna()\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df_filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRnYSze6xEJ7"
      },
      "outputs": [],
      "source": [
        "df['SEVERE_ACCIDENT'] = (df['NUMBER OF PERSONS INJURED'] > 0) | (df['NUMBER OF PERSONS KILLED'] > 0)\n",
        "df['SEVERE_ACCIDENT'] = df['SEVERE_ACCIDENT'].astype(int)\n",
        "\n",
        "df['BOROUGH'] = df['BOROUGH_BROOKLYN'] | df['BOROUGH_MANHATTAN'] | df['BOROUGH_QUEENS'] | df['BOROUGH_STATEN ISLAND']\n",
        "\n",
        "df['CRASH TIME'] = pd.to_datetime(df['CRASH TIME'])\n",
        "df['CRASH HOUR'] = df['CRASH TIME'].dt.hour\n",
        "\n",
        "features = [\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1', 'BOROUGH', 'CRASH HOUR',\n",
        "    'VEHICLE TYPE CODE 1', 'LATITUDE', 'LONGITUDE'\n",
        "]\n",
        "X = df[features]\n",
        "y = df['SEVERE_ACCIDENT']\n",
        "\n",
        "# Preprocessing pipeline\n",
        "numeric_features = ['CRASH HOUR', 'LATITUDE', 'LONGITUDE']\n",
        "categorical_features = ['CONTRIBUTING FACTOR VEHICLE 1', 'BOROUGH', 'VEHICLE TYPE CODE 1']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEnkUqs3xEJ7"
      },
      "outputs": [],
      "source": [
        "#1 Logistic Regression\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "param_grid_lr = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'solver': ['lbfgs', 'liblinear']\n",
        "}\n",
        "\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "grid_lr = GridSearchCV(lr, param_grid_lr, cv=5, scoring='accuracy')\n",
        "grid_lr.fit(X_train_imputed, y_train)\n",
        "\n",
        "best_lr = grid_lr.best_estimator_\n",
        "y_pred_lr = best_lr.predict(X_test_imputed)\n",
        "confusion_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "\n",
        "print(\"Logistic Regression Best Parameters:\", grid_lr.best_params_)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"Logistic Regression ROC AUC:\", roc_auc_score(y_test, best_lr.predict_proba(X_test_imputed)[:, 1]))\n",
        "print(\"Logistic Regression F1 Score:\", f1_score(y_test, y_pred_lr))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_lr, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Logistic Regression Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "#better accuracy with the following:\n",
        "##LOGISTIC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "import seaborn as sb\n",
        "\n",
        "lr_params = {\n",
        "    'C': np.logspace(-3,3,7),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear'],\n",
        "    'class_weight': ['balanced', None]\n",
        "}\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train_imputed, y_train)\n",
        "y_pred = model.predict(X_test_imputed)\n",
        "confusion =confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, model.predict_proba(X_test_imputed)[:, 1]))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sb.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULthEMkhxEJ8"
      },
      "outputs": [],
      "source": [
        "#2 Decision Tree\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "param_grid_dt = {\n",
        "    'max_depth': [None, 3, 5, 7, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "grid_dt = GridSearchCV(dt, param_grid_dt, cv=5, scoring='accuracy')\n",
        "grid_dt.fit(X_train_imputed, y_train)\n",
        "\n",
        "best_dt = grid_dt.best_estimator_\n",
        "y_pred_dt = best_dt.predict(X_test_imputed)\n",
        "confusion_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "# Display results\n",
        "print(\"Decision Tree Best Parameters:\", grid_dt.best_params_)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(\"Decision Tree F1 Score:\", f1_score(y_test, y_pred_dt))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_dt, annot=True, fmt='d', cmap='Greens')\n",
        "plt.title('Decision Tree Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#better accuracy with the following\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = DecisionTreeClassifier(max_depth=5)\n",
        "model.fit(X_train_imputed, y_train)\n",
        "y_pred = model.predict(X_test_imputed)\n",
        "confusion =confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sb.heatmap(confusion, annot=True, fmt='d', cmap='Greens')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21r6NRLkxEJ8"
      },
      "outputs": [],
      "source": [
        "#3 XGB BOOST\n",
        "\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "grid_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=5, scoring='accuracy')\n",
        "grid_xgb.fit(X_train_imputed, y_train)\n",
        "best_xgb = grid_xgb.best_estimator_\n",
        "y_pred_xgb = best_xgb.predict(X_test_imputed)\n",
        "confusion_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "\n",
        "# Display results\n",
        "print(\"XGBoost Best Parameters:\", grid_xgb.best_params_)\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"XGBoost F1 Score:\", f1_score(y_test, y_pred_xgb))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_xgb, annot=True, fmt='d', cmap='Purples')\n",
        "plt.title('XGBoost Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwqIvJhuxEJ9"
      },
      "outputs": [],
      "source": [
        "#4 DBSCAN\n",
        "\n",
        "# Converting the scaled NumPy array back to a DataFrame\n",
        "df_scaled_df = pd.DataFrame(df_scaled, columns=df_filtered.columns)\n",
        "\n",
        "# sampling the data\n",
        "df_scaled_subset = df_scaled_df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "#parameters for scaled data\n",
        "#dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
        "# df_scaled_subset = df_scaled.sample(frac=0.1, random_state=42)\n",
        "\n",
        "eps_values = [0.5, 0.75]\n",
        "min_samples_values = [15, 25, 50, 75, 100]\n",
        "\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_labels = None\n",
        "\n",
        "# Create a DataFrame to store silhouette scores\n",
        "scores_df = pd.DataFrame(index=eps_values, columns=min_samples_values)\n",
        "\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(df_scaled_subset)\n",
        "        unique_labels = np.unique(labels)\n",
        "        # Check if a meaningful clustering is found (more than 1 cluster)\n",
        "        if len(unique_labels) > 1 and len(unique_labels) < len(df_scaled_subset):\n",
        "            score = silhouette_score(df_scaled_subset, labels)\n",
        "            scores_df.loc[eps, min_samples] = score\n",
        "            print(f\"eps: {eps}, min_samples: {min_samples}, silhouette score: {score:.3f}\")\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = {'eps': eps, 'min_samples': min_samples}\n",
        "                best_labels = labels\n",
        "        else:\n",
        "            scores_df.loc[eps, min_samples] = np.nan\n",
        "\n",
        "print(\"Best DBSCAN Parameters:\", best_params)\n",
        "print(\"Best Silhouette Score:\", best_score)\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(scores_df, annot=True, cmap='viridis', cbar_kws={'label': 'Silhouette Score'})\n",
        "plt.title('Silhouette Scores for Different eps and min_samples Values')\n",
        "plt.xlabel('min_samples')\n",
        "plt.ylabel('eps')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q88y8_xxEJ9"
      },
      "outputs": [],
      "source": [
        "#5 Neural Networks\n",
        "\n",
        "#takes around 40 minutes on GPU\n",
        "\n",
        "#if this takes too long, try the following:\n",
        "#model = keras.Sequential([\n",
        "#    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "#    layers.Dropout(0.2),\n",
        "#    layers.Dense(32, activation='relu'),\n",
        "#    layers.Dense(1, activation='sigmoid')\n",
        "#])\n",
        "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n",
        "\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    # Tune number of units in the first layer\n",
        "    model.add(layers.Dense(units=hp.Int('units_1', min_value=32, max_value=128, step=32),\n",
        "                           activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    # Tune dropout rate\n",
        "    model.add(layers.Dropout(rate=hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    # Tune number of units in the second layer\n",
        "    model.add(layers.Dense(units=hp.Int('units_2', min_value=16, max_value=64, step=16),\n",
        "                           activation='relu'))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "tuner = kt.Hyperband(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=2,\n",
        "                     factor=2,\n",
        "                     directory='nn_tuning',\n",
        "                     project_name='tune_nn',\n",
        "                     hyperband_iterations=1,\n",
        "                     overwrite = True)\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
        "\n",
        "best_nn_model = tuner.get_best_models(num_models=1)[0]\n",
        "loss, nn_accuracy = best_nn_model.evaluate(X_test, y_test)\n",
        "print(\"Neural Network Best Hyperparameters:\", tuner.get_best_hyperparameters()[0].values)\n",
        "print(\"Neural Network Accuracy:\", nn_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6eEYPcExEJ-"
      },
      "outputs": [],
      "source": [
        "#6 KMEANS\n",
        "inertia = []\n",
        "K_range = range(2, 10)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(df_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow graph\n",
        "plt.plot(K_range, inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.show()\n",
        "\n",
        "# Reduce dimensions to 2D for visualization\n",
        "#pca = PCA(n_components=2)\n",
        "#df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "k_range = range(3, 10)\n",
        "scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(df_scaled)\n",
        "    score = silhouette_score(df_scaled, labels)\n",
        "    scores.append({k: score})\n",
        "    print(f\"Silhouette score for k={k}: {score:.3f}\")\n",
        "\n",
        "optimal_k = max(scores, key=lambda x: list(x.values())[0])\n",
        "optimal_k = list(optimal_k.keys())[0]\n",
        "print(f\"Optimal k: {optimal_k}\")\n",
        "\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "kmeans.fit(df_scaled)\n",
        "\n",
        "# Reduce dimensions to 2D for visualization\n",
        "pca = PCA(n_components=2)\n",
        "df_pca = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Add cluster labels to PCA dataframe\n",
        "df_plot = pd.DataFrame(df_pca, columns=['PC1', 'PC2'])\n",
        "df_plot['Cluster'] = kmeans.labels_\n",
        "\n",
        "# Plot the clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "for cluster in range(optimal_k):\n",
        "    cluster_data = df_plot[df_plot['Cluster'] == cluster]\n",
        "    plt.scatter(cluster_data['PC1'], cluster_data['PC2'], label=f'Cluster {cluster}')\n",
        "\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.title('Clusters Visualization')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ztr8mFi9xEJ-"
      },
      "outputs": [],
      "source": [
        "#7 KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier  # New import\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Initialize and train KNN classifier\n",
        "model = KNeighborsClassifier(\n",
        "    n_neighbors=10,    # Number of neighbors to consider\n",
        "    weights='uniform', # 'uniform' or 'distance' based weighting\n",
        "    algorithm='auto'  # Auto-choose best algorithm\n",
        ")\n",
        "model.fit(X_train_imputed, y_train)\n",
        "y_pred = model.predict(X_test_imputed)\n",
        "\n",
        "# Generate confusion matrix\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sb.heatmap(confusion, annot=True, fmt='d', cmap='Reds')\n",
        "plt.title('Confusion Matrix - KNN')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3G3qh7WxEJ_"
      },
      "outputs": [],
      "source": [
        "#8 Gradient Boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier  # New import\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "# Initialize and train Gradient Boosting model\n",
        "model = GradientBoostingClassifier(\n",
        "    n_estimators=100,  # Number of boosting stages\n",
        "    learning_rate=0.1,  # Shrinkage factor\n",
        "    max_depth=3,       # Maximum depth of individual trees\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train_imputed, y_train)\n",
        "y_pred = model.predict(X_test_imputed)\n",
        "\n",
        "# Generate confusion matrix\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "# Plot confusion matrix (same visualization)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sb.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Gradient Boosting')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipXo6OTIxEJ_"
      },
      "outputs": [],
      "source": [
        "#9 SVC\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "param_grid_svc = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto', 0.1, 1, 10],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "svc = SVC(probability=True)\n",
        "grid_svc = GridSearchCV(svc, param_grid_svc, cv=5, scoring='accuracy')\n",
        "grid_svc.fit(X_train_imputed, y_train)\n",
        "\n",
        "best_svc = grid_svc.best_estimator_\n",
        "y_pred_svc = best_svc.predict(X_test_imputed)\n",
        "confusion_svc = confusion_matrix(y_test, y_pred_svc)\n",
        "\n",
        "print(\"SVC Best Parameters:\", grid_svc.best_params_)\n",
        "print(\"SVC Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
        "print(\"SVC F1 Score:\", f1_score(y_test, y_pred_svc))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_svc, annot=True, fmt='d', cmap='Reds')\n",
        "plt.title('SVC Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "drive.mount('/content/drive')\n",
        "plt.show()"
      ]
    }
  ]
}