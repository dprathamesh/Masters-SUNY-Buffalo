{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3OehMO_3rj9"
      },
      "source": [
        "# **Phase 3 - Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_9Uhwm6JdHi"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, when, to_date, hour, dayofweek, year, month, regexp_replace, trim, udf, lag, sum as spark_sum\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import median, avg, stddev, min, max\n",
        "from pyspark.sql.functions import lower, upper, trim, lag, lead, avg, stddev, rank, dense_rank\n",
        "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
        "from pyspark.ml.functions import vector_to_array\n",
        "from pyspark.sql.functions import year, month, col, lag, lead, when, avg, rank\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import IntegerType, DoubleType, DateType, TimestampType,StringType\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myu8OaoPyi_0"
      },
      "source": [
        "**Cleaning 1: Handling Missing Values**\n",
        "\n",
        "With this cleaning step, we handled missing values in the dataset. Replacing missing numeric values in specified columns with 0 and fills missing categorical values in specified columns with 'Unspecified'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUyoeTL8KCvi"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 1: ANALYZE AND HANDLE MISSING VALUES\n",
        "# ===============================================================================\n",
        "# Purpose: We wanted to identify columns with missing values and handle them correctly.\n",
        "# Impact: It improved data quality and prevented failures in further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O9Xyl684_Pq",
        "outputId": "8fbda649-04a7-433e-b64d-6510e95afbf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset dimensions: 2146665 rows x 29 columns\n",
            "Missing values per column:\n",
            "+----------+----------+-------+--------+--------+---------+--------+--------------+-----------------+---------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|CRASH DATE|CRASH TIME|BOROUGH|ZIP CODE|LATITUDE|LONGITUDE|LOCATION|ON STREET NAME|CROSS STREET NAME|OFF STREET NAME|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|NUMBER OF PEDESTRIANS INJURED|NUMBER OF PEDESTRIANS KILLED|NUMBER OF CYCLIST INJURED|NUMBER OF CYCLIST KILLED|NUMBER OF MOTORIST INJURED|NUMBER OF MOTORIST KILLED|CONTRIBUTING FACTOR VEHICLE 1|CONTRIBUTING FACTOR VEHICLE 2|CONTRIBUTING FACTOR VEHICLE 3|CONTRIBUTING FACTOR VEHICLE 4|CONTRIBUTING FACTOR VEHICLE 5|COLLISION_ID|VEHICLE TYPE CODE 1|VEHICLE TYPE CODE 2|VEHICLE TYPE CODE 3|VEHICLE TYPE CODE 4|VEHICLE TYPE CODE 5|\n",
            "+----------+----------+-------+--------+--------+---------+--------+--------------+-----------------+---------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|         0|         0| 665561|  665824|  239544|   239544|  239544|        460905|           818426|        1778132|                       19|                      33|                            2|                           2|                        1|                       1|                         1|                        2|                         7298|                       338549|                      1992089|                      2111506|                      2137060|           2|              14893|             420434|            1997908|            2112764|            2137356|\n",
            "+----------+----------+-------+--------+--------+---------+--------+--------------+-----------------+---------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "Columns with missing values (percentage):\n",
            "+----------+----------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|CRASH DATE|CRASH TIME|          BOROUGH|          ZIP CODE|          LATITUDE|         LONGITUDE|          LOCATION|   ON STREET NAME| CROSS STREET NAME|  OFF STREET NAME|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|NUMBER OF PEDESTRIANS INJURED|NUMBER OF PEDESTRIANS KILLED|NUMBER OF CYCLIST INJURED|NUMBER OF CYCLIST KILLED|NUMBER OF MOTORIST INJURED|NUMBER OF MOTORIST KILLED|CONTRIBUTING FACTOR VEHICLE 1|CONTRIBUTING FACTOR VEHICLE 2|CONTRIBUTING FACTOR VEHICLE 3|CONTRIBUTING FACTOR VEHICLE 4|CONTRIBUTING FACTOR VEHICLE 5|        COLLISION_ID|VEHICLE TYPE CODE 1|VEHICLE TYPE CODE 2|VEHICLE TYPE CODE 3|VEHICLE TYPE CODE 4|VEHICLE TYPE CODE 5|\n",
            "+----------+----------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|       0.0|       0.0|31.00441848169137|31.016670043998477|11.158890651312618|11.158890651312618|11.158890651312618|21.47074648349882|38.125464383124516|82.83230033563692|     8.850938548865333E-4|    0.001537268274276...|         9.316777419858245E-5|        9.316777419858245E-5|     4.658388709929122...|    4.658388709929122...|      4.658388709929122...|     9.316777419858245E-5|          0.33996920805062736|           15.770928393577947|            92.79924906773996|            98.36215711347603|            99.55256176441131|9.316777419858245E-5| 0.6937738305697442| 19.585449988703406|  93.07032070677074|  98.42075964344693|   99.5663505949927|\n",
            "+----------+----------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n",
            "Missing values per column after replacement:\n",
            "+----------+----------+-------+--------+--------+---------+--------+--------------+-----------------+---------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|CRASH DATE|CRASH TIME|BOROUGH|ZIP CODE|LATITUDE|LONGITUDE|LOCATION|ON STREET NAME|CROSS STREET NAME|OFF STREET NAME|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|NUMBER OF PEDESTRIANS INJURED|NUMBER OF PEDESTRIANS KILLED|NUMBER OF CYCLIST INJURED|NUMBER OF CYCLIST KILLED|NUMBER OF MOTORIST INJURED|NUMBER OF MOTORIST KILLED|CONTRIBUTING FACTOR VEHICLE 1|CONTRIBUTING FACTOR VEHICLE 2|CONTRIBUTING FACTOR VEHICLE 3|CONTRIBUTING FACTOR VEHICLE 4|CONTRIBUTING FACTOR VEHICLE 5|COLLISION_ID|VEHICLE TYPE CODE 1|VEHICLE TYPE CODE 2|VEHICLE TYPE CODE 3|VEHICLE TYPE CODE 4|VEHICLE TYPE CODE 5|\n",
            "+----------+----------+-------+--------+--------+---------+--------+--------------+-----------------+---------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|         0|         0| 665561|  665824|  239544|   239544|  239544|        460905|           818426|        1778132|                        0|                       0|                            2|                           2|                        1|                       1|                         1|                        2|                            0|                            0|                            0|                            0|                            0|           2|                  0|                  0|                  0|                  0|                  0|\n",
            "+----------+----------+-------+--------+--------+---------+--------+--------------+-----------------+---------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#pyspark 1\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()\n",
        "file_path = \"/path/to/csv/cleaned_dataset.csv\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "print(f\"Original dataset dimensions: {df.count()} rows x {len(df.columns)} columns\")\n",
        "\n",
        "missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
        "\n",
        "print(\"Missing values per column:\")\n",
        "missing_values.show()\n",
        "\n",
        "total_rows = df.count()\n",
        "missing_percentage = df.select([(count(when(col(c).isNull(), c)) / total_rows * 100).alias(c) for c in df.columns])\n",
        "\n",
        "print(\"Columns with missing values (percentage):\")\n",
        "missing_percentage.show()\n",
        "\n",
        "#replaced missing numeric values with 0\n",
        "numerical_cols_to_be_replaced_by_Zero  = ['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED']\n",
        "\n",
        "df = df.fillna({col_name: 0 for col_name in numerical_cols_to_be_replaced_by_Zero})\n",
        "\n",
        "#replaced missing categorical values (e.g., 'CONTRIBUTING FACTOR VEHICLE 1') with 'Unspecified'\n",
        "categorical_cols_to_be_replaced_by_unspecified = ['CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3',\n",
        "                   'CONTRIBUTING FACTOR VEHICLE 4', 'CONTRIBUTING FACTOR VEHICLE 5', 'VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2',\n",
        "                                                 'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5']\n",
        "df = df.fillna({col_name: 'Unspecified' for col_name in categorical_cols_to_be_replaced_by_unspecified})\n",
        "missing_values_after = df.select([(spark_sum(col(c).isNull().cast(\"int\"))).alias(c) for c in df.columns])\n",
        "print(\"Missing values per column after replacement:\")\n",
        "missing_values_after.show()\n",
        "\n",
        "# How it helps:\n",
        "# Prior to handling: Identifying missing data informs us which columns may cause issues in modeling.\n",
        "# After addressing: By Addressing missing data we make sure that our dataset is more reliable and it improves the quality of our analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OycPGGS6yi_0"
      },
      "source": [
        "**Cleaning 2: Dropping unnecessary colums and removing Duplicates**\n",
        "\n",
        "In this step, we dropped unnecessary columns. We dropped the Location column because we already have separate columns for Latitude and Longitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwn-kuRgh36j"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 2: DIMENSION REDUCTION (DROPPING COLUMNS AND DUPLICATES)\n",
        "# ===============================================================================\n",
        "# Purpose: We needed to remove unnecessary columns and duplicate records to reduce data volume\n",
        "# Impact: It improved processing efficiency in the distributed environment by reducing data size\n",
        "# This operation is particularly important for big data processing where reducing data size\n",
        "# directly impacts processing time and cluster resource utilization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbd576uR7fa1",
        "outputId": "0b2480ad-9aa8-4983-bbe5-5be12727e51c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows identified: 0\n",
            "Dataset dimensions after removing duplicates: 2146665 rows x 28 columns\n",
            "+----------+----------+---------+--------+----------+-----------+--------------------+--------------------+--------------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|CRASH DATE|CRASH TIME|  BOROUGH|ZIP CODE|  LATITUDE|  LONGITUDE|      ON STREET NAME|   CROSS STREET NAME|     OFF STREET NAME|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|NUMBER OF PEDESTRIANS INJURED|NUMBER OF PEDESTRIANS KILLED|NUMBER OF CYCLIST INJURED|NUMBER OF CYCLIST KILLED|NUMBER OF MOTORIST INJURED|NUMBER OF MOTORIST KILLED|CONTRIBUTING FACTOR VEHICLE 1|CONTRIBUTING FACTOR VEHICLE 2|CONTRIBUTING FACTOR VEHICLE 3|CONTRIBUTING FACTOR VEHICLE 4|CONTRIBUTING FACTOR VEHICLE 5|COLLISION_ID|VEHICLE TYPE CODE 1|VEHICLE TYPE CODE 2|VEHICLE TYPE CODE 3|VEHICLE TYPE CODE 4|VEHICLE TYPE CODE 5|\n",
            "+----------+----------+---------+--------+----------+-----------+--------------------+--------------------+--------------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "|04/27/2016|     17:11| BROOKLYN|   11217|40.6830004|-73.9734777|                 ...|                 ...|673 ATLANTIC AVEN...|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|     3438786|  PASSENGER VEHICLE|  PASSENGER VEHICLE|        Unspecified|        Unspecified|        Unspecified|\n",
            "|03/16/2016|     10:51|MANHATTAN|   10019|40.7662862|-73.9818487|WEST 57 STREET   ...|BROADWAY         ...|                 ...|                        1|                       0|                            1|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|     3408022|  PASSENGER VEHICLE|        Unspecified|        Unspecified|        Unspecified|        Unspecified|\n",
            "|03/22/2016|     21:23|MANHATTAN|   10029|40.7992856|-73.9453602|MADISON AVENUE   ...|EAST 115 STREET  ...|                 ...|                        0|                       1|                            0|                           1|                        0|                       0|                         0|                        0|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|     3409398|  PASSENGER VEHICLE|        Unspecified|        Unspecified|        Unspecified|        Unspecified|\n",
            "|04/06/2016|      7:00|   QUEENS|   11356|40.7786204|-73.8414464|                 ...|                 ...|127-01 23 AVENUE ...|                        0|                       0|                            0|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|     3426562|  PASSENGER VEHICLE|        Unspecified|        Unspecified|        Unspecified|        Unspecified|\n",
            "|03/17/2016|     22:00|   QUEENS|   11105|40.7728532|-73.9060611|STEINWAY STREET  ...|DITMARS BOULEVARD...|                 ...|                        1|                       0|                            1|                           0|                        0|                       0|                         0|                        0|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|                  Unspecified|     3428709|  PASSENGER VEHICLE|        Unspecified|        Unspecified|        Unspecified|        Unspecified|\n",
            "+----------+----------+---------+--------+----------+-----------+--------------------+--------------------+--------------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Pyspark 2:\n",
        "\n",
        "#drropped the 'LOCATION' column\n",
        "df = df.drop('LOCATION')\n",
        "\n",
        "#iddentified and removed duplicate rows\n",
        "duplicate_count = df.count() - df.dropDuplicates().count()\n",
        "print(f\"Number of duplicate rows identified: {duplicate_count}\")\n",
        "\n",
        "#removed duplicate rows\n",
        "df = df.dropDuplicates()\n",
        "print(f\"Dataset dimensions after removing duplicates: {df.count()} rows x {len(df.columns)} columns\")\n",
        "\n",
        "#modified DataFrame\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxOTi9Wwyi_1"
      },
      "source": [
        "**Cleaning 3: Standardizing DateTime and making data types consistent**\n",
        "\n",
        "In this step, we standardized the values in Crash Date column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7r2Y-IWjozH"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 3: Standardizing Date Time and Data Types\n",
        "# ===============================================================================\n",
        "# Purpose: We made every date in a standard format and confirmed that data types are sufficient for the given data.\n",
        "# Impact: This step helped in standardizing the data, making it uniform for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbt-D9lJ7yrM",
        "outputId": "feadd1eb-7023-4cf1-d38a-17199b8c65a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Dates Summary:\n",
            "+-------------------+-------------------+\n",
            "|Missing Crash Dates|Missing Crash Times|\n",
            "+-------------------+-------------------+\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "|                  1|                  0|\n",
            "+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Schema after type conversion:\n",
            "root\n",
            " |-- CRASH DATE: date (nullable = true)\n",
            " |-- CRASH TIME: string (nullable = true)\n",
            " |-- BOROUGH: string (nullable = true)\n",
            " |-- ZIP CODE: string (nullable = true)\n",
            " |-- LATITUDE: double (nullable = true)\n",
            " |-- LONGITUDE: double (nullable = true)\n",
            " |-- ON STREET NAME: string (nullable = true)\n",
            " |-- CROSS STREET NAME: string (nullable = true)\n",
            " |-- OFF STREET NAME: string (nullable = true)\n",
            " |-- NUMBER OF PERSONS INJURED: double (nullable = true)\n",
            " |-- NUMBER OF PERSONS KILLED: double (nullable = false)\n",
            " |-- NUMBER OF PEDESTRIANS INJURED: double (nullable = true)\n",
            " |-- NUMBER OF PEDESTRIANS KILLED: double (nullable = true)\n",
            " |-- NUMBER OF CYCLIST INJURED: double (nullable = true)\n",
            " |-- NUMBER OF CYCLIST KILLED: double (nullable = true)\n",
            " |-- NUMBER OF MOTORIST INJURED: double (nullable = true)\n",
            " |-- NUMBER OF MOTORIST KILLED: double (nullable = true)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 1: string (nullable = false)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 2: string (nullable = false)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 3: string (nullable = false)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 4: string (nullable = false)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 5: string (nullable = false)\n",
            " |-- COLLISION_ID: integer (nullable = true)\n",
            " |-- VEHICLE TYPE CODE 1: string (nullable = false)\n",
            " |-- VEHICLE TYPE CODE 2: string (nullable = false)\n",
            " |-- VEHICLE TYPE CODE 3: string (nullable = false)\n",
            " |-- VEHICLE TYPE CODE 4: string (nullable = false)\n",
            " |-- VEHICLE TYPE CODE 5: string (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#converted date string to date type\n",
        "df = df.withColumn(\"CRASH DATE\", to_date(col(\"CRASH DATE\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "#checked for missing dates after conversion\n",
        "missing_dates = df.select(\n",
        "    col(\"CRASH DATE\").isNull().cast(\"int\").alias(\"Missing Crash Dates\"),\n",
        "    col(\"CRASH TIME\").isNull().cast(\"int\").alias(\"Missing Crash Times\")\n",
        ")\n",
        "print(\"Missing Dates Summary:\")\n",
        "missing_dates.show()\n",
        "\n",
        "#converted numeric columns to DoubleType for consistent numerical operations\n",
        "numeric_columns = [\n",
        "    'LATITUDE', 'LONGITUDE', 'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
        "    'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
        "    'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
        "    'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED'\n",
        "]\n",
        "for col_name in numeric_columns:\n",
        "    df = df.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
        "\n",
        "#converted categorical columns to StringType for consistent string operations\n",
        "categorical_columns = [\n",
        "    'CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4',\n",
        "    'CONTRIBUTING FACTOR VEHICLE 5', 'VEHICLE TYPE CODE 1',\n",
        "    'VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3',\n",
        "    'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5'\n",
        "]\n",
        "for col_name in categorical_columns:\n",
        "    df = df.withColumn(col_name, col(col_name).cast(StringType()))\n",
        "\n",
        "print(\"Schema after type conversion:\")\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjwtnkbDkrLP"
      },
      "source": [
        "**Cleaning 4: Outlier Detection and Removal**\n",
        "\n",
        "In this step, we detected outliers, those which could add bias to the data - removed them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co7IPBiH1kgE"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 4: OUTLIER DETECTION AND REMOVAL USING IQR METHOD\n",
        "# ===============================================================================\n",
        "# Purpose: Identified and removed statistical outliers from numerical columns\n",
        "# Impact: It improved model performance by removing extreme values that could skew analysis\n",
        "# Using distributed functions for statistical calculations is much more efficient\n",
        "# than collecting data to the driver and computing locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNxtaVG9yi_2",
        "outputId": "39a4a218-64e9-4d05-af8a-425e3fdcf4bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outlier bounds for NUMBER OF PERSONS INJURED: Lower=-1.5, Upper=2.5\n",
            "Number of outliers in NUMBER OF PERSONS INJURED: 39310\n"
          ]
        }
      ],
      "source": [
        "def detect_outliers_iqr(df, column):\n",
        "    \"\"\"\n",
        "    Detect outliers using the Interquartile Range (IQR) method\n",
        "    Returns the lower and upper bounds for filtering\n",
        "    \"\"\"\n",
        "    # Calculate Q1 (25th percentile) and Q3 (75th percentile) using approxQuantile\n",
        "    # This is a distributed operation that approximates quantiles efficiently\n",
        "    Q1, Q3 = df.approxQuantile(column, [0.25, 0.75], 0.05)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Apply outlier detection to injury columns\n",
        "injured_columns = ['NUMBER OF PERSONS INJURED']\n",
        "for col_name in injured_columns:\n",
        "    lower_bound, upper_bound = detect_outliers_iqr(df, col_name)\n",
        "    print(f\"Outlier bounds for {col_name}: Lower={lower_bound}, Upper={upper_bound}\")\n",
        "\n",
        "    # Count outliers before removal\n",
        "    outlier_count = df.filter((col(col_name) < lower_bound) | (col(col_name) > upper_bound)).count()\n",
        "    print(f\"Number of outliers in {col_name}: {outlier_count}\")\n",
        "\n",
        "    # Remove outliers\n",
        "    df = df.filter((col(col_name) >= lower_bound) & (col(col_name) <= upper_bound))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlBNjo1oyi_2"
      },
      "source": [
        "**Cleaning 5: Further DataType Consistency for Numeric data**\n",
        "\n",
        "In this step, we checked whether the required columns are of the appropriate datatype.\n",
        "\n",
        "For example, ensuring Number of Persons Injured is Numeric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTD-J35Tyi_2"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 5: TEXT STANDARDIZATION AND NORMALIZATION\n",
        "# ===============================================================================\n",
        "# Purpose: Normalize text fields for consistent analysis\n",
        "# Impact: Improved grouping and aggregation accuracy by handling inconsistent text values\n",
        "# Especially important for distributed processing where data comes from multiple sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUCElqow-dD2",
        "outputId": "5d5c7625-43cb-4e1a-de9e-651e1bf0b39c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample of standardized categorical columns:\n",
            "+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+\n",
            "|CONTRIBUTING FACTOR VEHICLE 1|CONTRIBUTING FACTOR VEHICLE 2|CONTRIBUTING FACTOR VEHICLE 3|CONTRIBUTING FACTOR VEHICLE 4|CONTRIBUTING FACTOR VEHICLE 5|\n",
            "+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+\n",
            "|                  unspecified|                  unspecified|                  unspecified|                  unspecified|                  unspecified|\n",
            "|                  unspecified|                  unspecified|                  unspecified|                  unspecified|                  unspecified|\n",
            "|                  unspecified|                  unspecified|                  unspecified|                  unspecified|                  unspecified|\n",
            "|                  unspecified|                  unspecified|                  unspecified|                  unspecified|                  unspecified|\n",
            "|                  unspecified|                  unspecified|                  unspecified|                  unspecified|                  unspecified|\n",
            "+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Standardize categorical columns (convert to lowercase and handle nulls)\n",
        "for col_name in categorical_columns:\n",
        "    df = df.withColumn(\n",
        "        col_name,\n",
        "        when(col(col_name).isNull(), 'unknown')\n",
        "        .otherwise(lower(col(col_name))).cast(\"string\")\n",
        "    )\n",
        "\n",
        "print(\"Sample of standardized categorical columns:\")\n",
        "df.select(categorical_columns[:5]).show(5)  # Show first two columns as example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYuxURVKnVlX"
      },
      "source": [
        "**Cleaning 6: Lat Long Validation**\n",
        "\n",
        "In this step, we put Lat Long values in an appropriate range\n",
        "To prevent the spatial analysis errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0r-j61L2G24"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 6: GEOGRAPHIC COORDINATE VALIDATION\n",
        "# ===============================================================================\n",
        "# Purpose: Ensure latitude and longitude values are within valid ranges\n",
        "# Impact: Prevents spatial analysis errors from invalid coordinates\n",
        "# This operation demonstrates data validation in a distributed context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjdN0_MNyi_3",
        "outputId": "83d9ad11-2416-4dc9-cbd6-8600a4022488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of records with invalid coordinates: 234699\n",
            "Sample of validated coordinates:\n",
            "+----------+-----------+\n",
            "|  LATITUDE|  LONGITUDE|\n",
            "+----------+-----------+\n",
            "|40.6830004|-73.9734777|\n",
            "|40.7662862|-73.9818487|\n",
            "|40.7992856|-73.9453602|\n",
            "|40.7786204|-73.8414464|\n",
            "|40.7728532|-73.9060611|\n",
            "+----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = df.withColumn(\n",
        "    'LATITUDE',\n",
        "    when((col('LATITUDE') >= -90) & (col('LATITUDE') <= 90), col('LATITUDE')).otherwise(None)\n",
        ")\n",
        "\n",
        "df = df.withColumn(\n",
        "    'LONGITUDE',\n",
        "    when((col('LONGITUDE') >= -180) & (col('LONGITUDE') <= 180), col('LONGITUDE')).otherwise(None)\n",
        ")\n",
        "\n",
        "# Count invalid coordinates that were fixed\n",
        "invalid_coords = df.filter(col('LATITUDE').isNull() | col('LONGITUDE').isNull()).count()\n",
        "print(f\"Number of records with invalid coordinates: {invalid_coords}\")\n",
        "print(\"Sample of validated coordinates:\")\n",
        "df.select(\"LATITUDE\", \"LONGITUDE\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxLiJGtFyi_3"
      },
      "source": [
        "**Cleaning 7: Scaling Features**\n",
        "\n",
        "In this step, we scaled our features to bring them in an appropriate range using min-max scaler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNOeVUjI2qBu"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 7: FEATURE SCALING USING MIN-MAX SCALING\n",
        "# ===============================================================================\n",
        "# Purpose: Aim was to normalize numerical features to a standard range (0-1)\n",
        "# Impact: It definitely enhances the comparability between features with different scales\n",
        "# Demonstrates ML pipeline integration with data cleaning in distributed environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfbvN6eOyi_3"
      },
      "outputs": [],
      "source": [
        "# we can also save this updated data into a csv using the following command -\n",
        "# data.to_csv('cleaned_dataset_with_standardized_text.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OnRK-nw_YAk",
        "outputId": "00a4f099-f575-4a65-8229-ff9db497c190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample of scaled features:\n",
            "+-------------------------+------------------------+--------------------------------+-------------------------------+\n",
            "|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|NUMBER OF PERSONS INJURED_SCALED|NUMBER OF PERSONS KILLED_SCALED|\n",
            "+-------------------------+------------------------+--------------------------------+-------------------------------+\n",
            "|                      0.0|                     0.0|                             0.0|                            0.0|\n",
            "|                      1.0|                     0.0|                             0.5|                            0.0|\n",
            "|                      0.0|                     1.0|                             0.0|                            0.2|\n",
            "|                      0.0|                     0.0|                             0.0|                            0.0|\n",
            "|                      1.0|                     0.0|                             0.5|                            0.0|\n",
            "+-------------------------+------------------------+--------------------------------+-------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "columns_to_scale = ['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED']\n",
        "\n",
        "#dropped the 'features' column if it already exists\n",
        "if \"features\" in df.columns:\n",
        "    df = df.drop(\"features\")\n",
        "\n",
        "#assembled the specified columns into a feature vector\n",
        "assembler = VectorAssembler(inputCols=columns_to_scale, outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "#intialized the MinMaxScaler to scale the 'features' column\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "scaler_model = scaler.fit(df)\n",
        "df_scaled = scaler_model.transform(df)\n",
        "\n",
        "#converted the vector column to an array so that we can extract elements\n",
        "df_scaled = df_scaled.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n",
        "\n",
        "#extracted the individual scaled columns and add \"_SCALED\" suffix\n",
        "df_scaled = df_scaled.withColumn(\"NUMBER OF PERSONS INJURED_SCALED\", col(\"scaled_array\")[0]) \\\n",
        "                     .withColumn(\"NUMBER OF PERSONS KILLED_SCALED\", col(\"scaled_array\")[1]) \\\n",
        "                     .drop(\"features\", \"scaled_features\", \"scaled_array\")\n",
        "\n",
        "print(\"Sample of scaled features:\")\n",
        "df_scaled.select(columns_to_scale + [\"NUMBER OF PERSONS INJURED_SCALED\", \"NUMBER OF PERSONS KILLED_SCALED\"]).show(5)\n",
        "\n",
        "df = df_scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X8fr25Xyi_3"
      },
      "source": [
        "**Cleaning 8: Handling critical columns by adding new feature/s**\n",
        "\n",
        "In this step, we are creating a new category which will classify each incident based on how many people were injured in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9xPM-Ik22Ww"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 8: FEATURE ENGINEERING - DERIVED METRICS\n",
        "# ===============================================================================\n",
        "# Purpose: Created new columns with derived metrics for enhanced analysis\n",
        "# Impact: Enables deeper insights by calculating compound metrics from raw data\n",
        "# This shows how distributed processing can create and validate new features efficiently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUWja_Qm_h0k",
        "outputId": "576a4dff-5e36-4e5d-a33f-adfa15006ad2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Injuries: 1072282.0\n",
            "Total Deaths: 6104.0\n",
            "Inconsistent Total Affected Rows: 0\n",
            "Data is consistent\n"
          ]
        }
      ],
      "source": [
        "#new column creation by aggregating injury and death metrics\n",
        "df = df.withColumn(\n",
        "    \"Total Injuries\",\n",
        "    col(\"NUMBER OF PERSONS INJURED\") +\n",
        "    col(\"NUMBER OF PEDESTRIANS INJURED\") +\n",
        "    col(\"NUMBER OF CYCLIST INJURED\") +\n",
        "    col(\"NUMBER OF MOTORIST INJURED\")\n",
        ")\n",
        "\n",
        "df = df.withColumn(\n",
        "    \"Total Deaths\",\n",
        "    col(\"NUMBER OF PERSONS KILLED\") +\n",
        "    col(\"NUMBER OF PEDESTRIANS KILLED\") +\n",
        "    col(\"NUMBER OF CYCLIST KILLED\") +\n",
        "    col(\"NUMBER OF MOTORIST KILLED\")\n",
        ")\n",
        "\n",
        "df = df.withColumn(\"Total Affected\", col(\"Total Injuries\") + col(\"Total Deaths\"))\n",
        "\n",
        "\n",
        "total_injuries = df.agg(F.sum(\"Total Injuries\").alias(\"total_injuries\")).collect()[0][\"total_injuries\"]\n",
        "total_deaths = df.agg(F.sum(\"Total Deaths\").alias(\"total_deaths\")).collect()[0][\"total_deaths\"]\n",
        "\n",
        "print(\"Total Injuries:\", total_injuries)\n",
        "print(\"Total Deaths:\", total_deaths)\n",
        "\n",
        "\n",
        "df = df.withColumn(\"Total Affected Check\", col(\"Total Injuries\") + col(\"Total Deaths\"))\n",
        "inconsistent_total_affected = df.filter(col(\"Total Affected\") != col(\"Total Affected Check\"))\n",
        "inconsistent_count = inconsistent_total_affected.count()\n",
        "\n",
        "print(\"Inconsistent Total Affected Rows:\", inconsistent_count)\n",
        "if inconsistent_count == 0:\n",
        "    print(\"Data is consistent\")\n",
        "else:\n",
        "    print(\"Data inconsistencies found:\")\n",
        "    inconsistent_total_affected.select(\"Total Affected\", \"Total Affected Check\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QzCK6rfv3P9"
      },
      "source": [
        "**Cleaning 9 & 10: Extracting features to perform Time Series Analysis**\n",
        "\n",
        "Analyzing and investigating interesting trends that move with time (For e.g. - seasonal change in the accidents)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EooDTU7Wyi_4"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 9: TEMPORAL FEATURE EXTRACTION\n",
        "# ===============================================================================\n",
        "# Purpose: To Extract date-based features for time-based analysis\n",
        "# Impact: This step enabled seasonal trend analysis and time-based aggregations\n",
        "# Demonstrates date manipulation in a distributed context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZHxe7OzHMRz",
        "outputId": "701043d3-cca1-4bf2-9b98-039d8f1ec3ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample of temporal features:\n",
            "+----------+----+-----+-----------+------+\n",
            "|CRASH DATE|YEAR|MONTH|DAY_OF_WEEK|SEASON|\n",
            "+----------+----+-----+-----------+------+\n",
            "|      NULL|NULL| NULL|       NULL|  Fall|\n",
            "|      NULL|NULL| NULL|       NULL|  Fall|\n",
            "|      NULL|NULL| NULL|       NULL|  Fall|\n",
            "|      NULL|NULL| NULL|       NULL|  Fall|\n",
            "|      NULL|NULL| NULL|       NULL|  Fall|\n",
            "+----------+----+-----+-----------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#new year, month, day columns\n",
        "df = df.withColumn(\"YEAR\", year(col(\"CRASH DATE\")))\n",
        "df = df.withColumn(\"MONTH\", month(col(\"CRASH DATE\")))\n",
        "df = df.withColumn(\"DAY_OF_WEEK\", dayofweek(col(\"CRASH DATE\")))\n",
        "\n",
        "#new column \"Season\" based on the month\n",
        "df = df.withColumn(\"SEASON\",\n",
        "    when(month(\"CRASH DATE\").isin([12, 1, 2]), \"Winter\")\n",
        "    .when(month(\"CRASH DATE\").isin([3, 4, 5]), \"Spring\")\n",
        "    .when(month(\"CRASH DATE\").isin([6, 7, 8]), \"Summer\")\n",
        "    .otherwise(\"Fall\")\n",
        ")\n",
        "\n",
        "print(\"Sample of temporal features:\")\n",
        "df.select(\"CRASH DATE\", \"YEAR\", \"MONTH\", \"DAY_OF_WEEK\", \"SEASON\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FdY8Edt3CMX"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 10: WINDOW FUNCTIONS FOR TIME-SERIES ANALYSIS (EXPLICITLY REQUIRED)\n",
        "# ===============================================================================\n",
        "# Purpose: We thought of using the window functions to analyze trends over time and within boroughs\n",
        "# Impact: Enables advanced time-series analysis like moving averages and rankings\n",
        "# This directly addresses the assignment requirement for windowing techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kNK2ISLApRx",
        "outputId": "f9e75380-0bb4-4df6-9730-214f5b14e573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample window function results:\n",
            "+-------+----------+-------------------------+-----------------+-------------+-------------------+-----------------+\n",
            "|BOROUGH|CRASH DATE|NUMBER OF PERSONS INJURED|PREV_DAY_INJURIES|INJURY_CHANGE|BOROUGH_INJURY_RANK|7DAY_AVG_INJURIES|\n",
            "+-------+----------+-------------------------+-----------------+-------------+-------------------+-----------------+\n",
            "|   NULL|      NULL|                      2.0|              1.0|          1.0|                  1|              2.0|\n",
            "|   NULL|      NULL|                      2.0|              0.0|          2.0|                  1|              2.0|\n",
            "|   NULL|      NULL|                      2.0|              0.0|          2.0|                  1|              2.0|\n",
            "|   NULL|      NULL|                      2.0|              1.0|          1.0|                  1|              2.0|\n",
            "|   NULL|      NULL|                      2.0|              0.0|          2.0|                  1|              2.0|\n",
            "+-------+----------+-------------------------+-----------------+-------------+-------------------+-----------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Sample month-over-month analysis:\n",
            "+---------+----+-----+----------------+-------------------+------------------+\n",
            "|  BOROUGH|YEAR|MONTH|MONTHLY_INJURIES|PREV_MONTH_INJURIES|MONTHLY_CHANGE_PCT|\n",
            "+---------+----+-----+----------------+-------------------+------------------+\n",
            "|     NULL|NULL| NULL|        177604.0|               NULL|              NULL|\n",
            "|    BRONX|NULL| NULL|         57591.0|               NULL|              NULL|\n",
            "| BROOKLYN|NULL| NULL|        128465.0|               NULL|              NULL|\n",
            "|MANHATTAN|NULL| NULL|         63217.0|               NULL|              NULL|\n",
            "|   QUEENS|NULL| NULL|         99273.0|               NULL|              NULL|\n",
            "+---------+----+-----+----------------+-------------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#windowing is a prominent technique used in Spark. it has a huge potential to increase data processing.\n",
        "\n",
        "#a window specification partitioned by borough and ordered by date\n",
        "borough_window = Window.partitionBy(\"BOROUGH\").orderBy(\"CRASH DATE\")\n",
        "\n",
        "#a month-level window for seasonal analysis\n",
        "month_window = Window.partitionBy(\"BOROUGH\", \"MONTH\").orderBy(\"CRASH DATE\")\n",
        "\n",
        "#rolling metrics using window functions\n",
        "df = df.withColumn(\"PREV_DAY_INJURIES\", lag(\"NUMBER OF PERSONS INJURED\", 1).over(borough_window))\n",
        "df = df.withColumn(\"NEXT_DAY_INJURIES\", lead(\"NUMBER OF PERSONS INJURED\", 1).over(borough_window))\n",
        "df = df.withColumn(\"INJURY_CHANGE\", col(\"NUMBER OF PERSONS INJURED\") - col(\"PREV_DAY_INJURIES\"))\n",
        "\n",
        "#ranking of accidents by borough\n",
        "df = df.withColumn(\"BOROUGH_INJURY_RANK\", rank().over(Window.partitionBy(\"BOROUGH\").orderBy(col(\"NUMBER OF PERSONS INJURED\").desc())))\n",
        "\n",
        "#moving averages for injuries (7-day window)\n",
        "seven_day_window = Window.partitionBy(\"BOROUGH\").orderBy(\"CRASH DATE\").rowsBetween(-6, 0)\n",
        "df = df.withColumn(\"7DAY_AVG_INJURIES\", avg(\"NUMBER OF PERSONS INJURED\").over(seven_day_window))\n",
        "\n",
        "df = df.withColumn(\"YEAR\", year(col(\"CRASH DATE\")))\n",
        "df = df.withColumn(\"MONTH\", month(col(\"CRASH DATE\")))\n",
        "\n",
        "\n",
        "monthly_agg = df.groupBy(\"BOROUGH\", \"YEAR\", \"MONTH\").agg(\n",
        "    F.sum(\"NUMBER OF PERSONS INJURED\").alias(\"MONTHLY_INJURIES\"),\n",
        "    F.countDistinct(\"CRASH DATE\").alias(\"ACCIDENT_DAYS\")\n",
        ")\n",
        "\n",
        "#window for month-over-month comparison\n",
        "month_comparison_window = Window.partitionBy(\"BOROUGH\").orderBy(\"YEAR\", \"MONTH\")\n",
        "\n",
        "#month-over-month changes\n",
        "monthly_agg = monthly_agg.withColumn(\n",
        "    \"PREV_MONTH_INJURIES\",\n",
        "    lag(\"MONTHLY_INJURIES\", 1).over(month_comparison_window)\n",
        ")\n",
        "\n",
        "monthly_agg = monthly_agg.withColumn(\n",
        "    \"MONTHLY_CHANGE_PCT\",\n",
        "    when(col(\"PREV_MONTH_INJURIES\").isNotNull() & (col(\"PREV_MONTH_INJURIES\") > 0),\n",
        "         ((col(\"MONTHLY_INJURIES\") - col(\"PREV_MONTH_INJURIES\")) / col(\"PREV_MONTH_INJURIES\") * 100)\n",
        "    ).otherwise(None)\n",
        ")\n",
        "\n",
        "print(\"Sample window function results:\")\n",
        "df.select(\"BOROUGH\", \"CRASH DATE\", \"NUMBER OF PERSONS INJURED\", \"PREV_DAY_INJURIES\",\n",
        "          \"INJURY_CHANGE\", \"BOROUGH_INJURY_RANK\", \"7DAY_AVG_INJURIES\").show(5)\n",
        "\n",
        "print(\"Sample month-over-month analysis:\")\n",
        "monthly_agg.select(\"BOROUGH\", \"YEAR\", \"MONTH\", \"MONTHLY_INJURIES\",\n",
        "                   \"PREV_MONTH_INJURIES\", \"MONTHLY_CHANGE_PCT\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-vEfxIiwluS"
      },
      "source": [
        "**Cleaning 11: Flagging the critical features**\n",
        "Our ultimate goal is to create an automated recommender system and colaborate with the traffic control department of NYS.\n",
        "For that, we are providing Green (Improvements) and Red (Require immediate attention) flasgs for different categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxSGGXJTyi_5"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 11: DATA CATEGORIZATION AND FLAGGING\n",
        "# ===============================================================================\n",
        "# Purpose: Create categorical features and flags for filtering and aggregation\n",
        "# Impact: Simplifies downstream analysis with pre-computed categories and flags\n",
        "# Shows practical application of conditional logic in distributed data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ1qZsNe3N-s",
        "outputId": "78b41779-aeff-46b0-d72d-0293db3e8a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample categorization and flags:\n",
            "+----------+-----------+-------------------------+----------------+-----------------+------------+--------------+\n",
            "|  LATITUDE|  LONGITUDE|NUMBER OF PERSONS INJURED|CRITICAL_MISSING|ACCIDENT_SEVERITY|HAS_INJURIES|HAS_FATALITIES|\n",
            "+----------+-----------+-------------------------+----------------+-----------------+------------+--------------+\n",
            "|40.6830004|-73.9734777|                      0.0|           false|              Low|       false|         false|\n",
            "|40.7662862|-73.9818487|                      1.0|           false|              Low|        true|         false|\n",
            "|40.7992856|-73.9453602|                      0.0|           false|              Low|       false|          true|\n",
            "|40.7786204|-73.8414464|                      0.0|           false|              Low|       false|         false|\n",
            "|40.7728532|-73.9060611|                      1.0|           false|              Low|        true|         false|\n",
            "+----------+-----------+-------------------------+----------------+-----------------+------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#critical missing data flag\n",
        "df = df.withColumn(\n",
        "    \"CRITICAL_MISSING\",\n",
        "    col(\"LATITUDE\").isNull() | col(\"LONGITUDE\").isNull() | col(\"NUMBER OF PERSONS INJURED\").isNull()\n",
        ")\n",
        "\n",
        "#accident severity categories\n",
        "df = df.withColumn(\n",
        "    \"ACCIDENT_SEVERITY\",\n",
        "    when(col(\"NUMBER OF PERSONS INJURED\") < 2, \"Low\")\n",
        "     .when((col(\"NUMBER OF PERSONS INJURED\") >= 2) & (col(\"NUMBER OF PERSONS INJURED\") < 4), \"Medium\")\n",
        "     .otherwise(\"High\")\n",
        ")\n",
        "\n",
        "#different types of flags for injuries and fatalities\n",
        "df = df.withColumn(\"HAS_INJURIES\", when(col(\"NUMBER OF PERSONS INJURED\") > 0, True).otherwise(False))\n",
        "df = df.withColumn(\"HAS_FATALITIES\", when(col(\"NUMBER OF PERSONS KILLED\") > 0, True).otherwise(False))\n",
        "\n",
        "print(\"Sample categorization and flags:\")\n",
        "df.select(\"LATITUDE\", \"LONGITUDE\", \"NUMBER OF PERSONS INJURED\",\n",
        "          \"CRITICAL_MISSING\", \"ACCIDENT_SEVERITY\", \"HAS_INJURIES\", \"HAS_FATALITIES\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BULebhdqyi_5"
      },
      "source": [
        "**Cleaning 12: Seasonal Tracking**\n",
        "\n",
        "In this step, we are categorizing our incidents based on the season. Since our data does not have any data related to the weather conditions, this will allow us to approximate the conditions for each incident."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzyVE564x6_B"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "# OPERATION 12: Season based analysis of crashes\n",
        "# ===============================================================================\n",
        "# Purpose: Categorize incidents based on season\n",
        "# Impact: It helped us approximate the conditions for every incident (crash / injury / casualty)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep4hcvMHHldM",
        "outputId": "10d1106f-9f4b-45d3-8ec7-c130d89d5fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Row(CRASH DATE=None, CRASH TIME='0:00', BOROUGH=None, ZIP CODE=None, LATITUDE=None, LONGITUDE=None, ON STREET NAME='BELT PARKWAY                    ', CROSS STREET NAME=None, OFF STREET NAME=None, NUMBER OF PERSONS INJURED=2.0, NUMBER OF PERSONS KILLED=0.0, NUMBER OF PEDESTRIANS INJURED=0.0, NUMBER OF PEDESTRIANS KILLED=0.0, NUMBER OF CYCLIST INJURED=0.0, NUMBER OF CYCLIST KILLED=0.0, NUMBER OF MOTORIST INJURED=2.0, NUMBER OF MOTORIST KILLED=0.0, CONTRIBUTING FACTOR VEHICLE 1='following too closely', CONTRIBUTING FACTOR VEHICLE 2='unspecified', CONTRIBUTING FACTOR VEHICLE 3='unspecified', CONTRIBUTING FACTOR VEHICLE 4='unspecified', CONTRIBUTING FACTOR VEHICLE 5='unspecified', COLLISION_ID=4247787, VEHICLE TYPE CODE 1='sedan', VEHICLE TYPE CODE 2='unspecified', VEHICLE TYPE CODE 3='unspecified', VEHICLE TYPE CODE 4='unspecified', VEHICLE TYPE CODE 5='unspecified', NUMBER OF PERSONS INJURED_SCALED=1.0, NUMBER OF PERSONS KILLED_SCALED=0.0, Total Injuries=4.0, Total Deaths=0.0, Total Affected=4.0, Total Affected Check=4.0, YEAR=None, MONTH=None, DAY_OF_WEEK=None, SEASON='Fall', PREV_DAY_INJURIES=0.0, NEXT_DAY_INJURIES=1.0, INJURY_CHANGE=2.0, BOROUGH_INJURY_RANK=1, 7DAY_AVG_INJURIES=2.0, CRITICAL_MISSING=True, ACCIDENT_SEVERITY='Medium', HAS_INJURIES=True, HAS_FATALITIES=False), Row(CRASH DATE=None, CRASH TIME='3:44', BOROUGH=None, ZIP CODE=None, LATITUDE=None, LONGITUDE=None, ON STREET NAME='WEST STREET', CROSS STREET NAME=None, OFF STREET NAME=None, NUMBER OF PERSONS INJURED=0.0, NUMBER OF PERSONS KILLED=0.0, NUMBER OF PEDESTRIANS INJURED=0.0, NUMBER OF PEDESTRIANS KILLED=0.0, NUMBER OF CYCLIST INJURED=0.0, NUMBER OF CYCLIST KILLED=0.0, NUMBER OF MOTORIST INJURED=0.0, NUMBER OF MOTORIST KILLED=0.0, CONTRIBUTING FACTOR VEHICLE 1='unsafe lane changing', CONTRIBUTING FACTOR VEHICLE 2='unspecified', CONTRIBUTING FACTOR VEHICLE 3='unspecified', CONTRIBUTING FACTOR VEHICLE 4='unspecified', CONTRIBUTING FACTOR VEHICLE 5='unspecified', COLLISION_ID=4667074, VEHICLE TYPE CODE 1='station wagon/sport utility vehicle', VEHICLE TYPE CODE 2='station wagon/sport utility vehicle', VEHICLE TYPE CODE 3='unspecified', VEHICLE TYPE CODE 4='unspecified', VEHICLE TYPE CODE 5='unspecified', NUMBER OF PERSONS INJURED_SCALED=0.0, NUMBER OF PERSONS KILLED_SCALED=0.0, Total Injuries=0.0, Total Deaths=0.0, Total Affected=0.0, Total Affected Check=0.0, YEAR=None, MONTH=None, DAY_OF_WEEK=None, SEASON='Fall', PREV_DAY_INJURIES=1.0, NEXT_DAY_INJURIES=1.0, INJURY_CHANGE=-1.0, BOROUGH_INJURY_RANK=149396, 7DAY_AVG_INJURIES=1.0, CRITICAL_MISSING=True, ACCIDENT_SEVERITY='Low', HAS_INJURIES=False, HAS_FATALITIES=False), Row(CRASH DATE=None, CRASH TIME='22:30', BOROUGH=None, ZIP CODE=None, LATITUDE=40.844105, LONGITUDE=-73.898, ON STREET NAME='CROSS BRONX EXPY                ', CROSS STREET NAME=None, OFF STREET NAME=None, NUMBER OF PERSONS INJURED=2.0, NUMBER OF PERSONS KILLED=0.0, NUMBER OF PEDESTRIANS INJURED=0.0, NUMBER OF PEDESTRIANS KILLED=0.0, NUMBER OF CYCLIST INJURED=0.0, NUMBER OF CYCLIST KILLED=0.0, NUMBER OF MOTORIST INJURED=2.0, NUMBER OF MOTORIST KILLED=0.0, CONTRIBUTING FACTOR VEHICLE 1='driver inattention/distraction', CONTRIBUTING FACTOR VEHICLE 2='unspecified', CONTRIBUTING FACTOR VEHICLE 3='unspecified', CONTRIBUTING FACTOR VEHICLE 4='unspecified', CONTRIBUTING FACTOR VEHICLE 5='unspecified', COLLISION_ID=3977229, VEHICLE TYPE CODE 1='sedan', VEHICLE TYPE CODE 2='unspecified', VEHICLE TYPE CODE 3='unspecified', VEHICLE TYPE CODE 4='unspecified', VEHICLE TYPE CODE 5='unspecified', NUMBER OF PERSONS INJURED_SCALED=1.0, NUMBER OF PERSONS KILLED_SCALED=0.0, Total Injuries=4.0, Total Deaths=0.0, Total Affected=4.0, Total Affected Check=4.0, YEAR=None, MONTH=None, DAY_OF_WEEK=None, SEASON='Fall', PREV_DAY_INJURIES=1.0, NEXT_DAY_INJURIES=0.0, INJURY_CHANGE=1.0, BOROUGH_INJURY_RANK=1, 7DAY_AVG_INJURIES=1.3333333333333333, CRITICAL_MISSING=False, ACCIDENT_SEVERITY='Medium', HAS_INJURIES=True, HAS_FATALITIES=False), Row(CRASH DATE=None, CRASH TIME='19:45', BOROUGH=None, ZIP CODE=None, LATITUDE=None, LONGITUDE=None, ON STREET NAME=None, CROSS STREET NAME=None, OFF STREET NAME=None, NUMBER OF PERSONS INJURED=0.0, NUMBER OF PERSONS KILLED=0.0, NUMBER OF PEDESTRIANS INJURED=0.0, NUMBER OF PEDESTRIANS KILLED=0.0, NUMBER OF CYCLIST INJURED=0.0, NUMBER OF CYCLIST KILLED=0.0, NUMBER OF MOTORIST INJURED=0.0, NUMBER OF MOTORIST KILLED=0.0, CONTRIBUTING FACTOR VEHICLE 1='unspecified', CONTRIBUTING FACTOR VEHICLE 2='unspecified', CONTRIBUTING FACTOR VEHICLE 3='unspecified', CONTRIBUTING FACTOR VEHICLE 4='unspecified', CONTRIBUTING FACTOR VEHICLE 5='unspecified', COLLISION_ID=3046333, VEHICLE TYPE CODE 1='passenger vehicle', VEHICLE TYPE CODE 2='unknown', VEHICLE TYPE CODE 3='unspecified', VEHICLE TYPE CODE 4='unspecified', VEHICLE TYPE CODE 5='unspecified', NUMBER OF PERSONS INJURED_SCALED=0.0, NUMBER OF PERSONS KILLED_SCALED=0.0, Total Injuries=0.0, Total Deaths=0.0, Total Affected=0.0, Total Affected Check=0.0, YEAR=None, MONTH=None, DAY_OF_WEEK=None, SEASON='Fall', PREV_DAY_INJURIES=0.0, NEXT_DAY_INJURIES=1.0, INJURY_CHANGE=0.0, BOROUGH_INJURY_RANK=149396, 7DAY_AVG_INJURIES=1.0, CRITICAL_MISSING=True, ACCIDENT_SEVERITY='Low', HAS_INJURIES=False, HAS_FATALITIES=False), Row(CRASH DATE=None, CRASH TIME='0:30', BOROUGH=None, ZIP CODE=None, LATITUDE=40.58584, LONGITUDE=-73.93788, ON STREET NAME='BELT PARKWAY                    ', CROSS STREET NAME=None, OFF STREET NAME=None, NUMBER OF PERSONS INJURED=2.0, NUMBER OF PERSONS KILLED=0.0, NUMBER OF PEDESTRIANS INJURED=0.0, NUMBER OF PEDESTRIANS KILLED=0.0, NUMBER OF CYCLIST INJURED=0.0, NUMBER OF CYCLIST KILLED=0.0, NUMBER OF MOTORIST INJURED=2.0, NUMBER OF MOTORIST KILLED=0.0, CONTRIBUTING FACTOR VEHICLE 1='driver inexperience', CONTRIBUTING FACTOR VEHICLE 2='unspecified', CONTRIBUTING FACTOR VEHICLE 3='unspecified', CONTRIBUTING FACTOR VEHICLE 4='unspecified', CONTRIBUTING FACTOR VEHICLE 5='unspecified', COLLISION_ID=4389981, VEHICLE TYPE CODE 1='station wagon/sport utility vehicle', VEHICLE TYPE CODE 2='station wagon/sport utility vehicle', VEHICLE TYPE CODE 3='unspecified', VEHICLE TYPE CODE 4='unspecified', VEHICLE TYPE CODE 5='unspecified', NUMBER OF PERSONS INJURED_SCALED=1.0, NUMBER OF PERSONS KILLED_SCALED=0.0, Total Injuries=4.0, Total Deaths=0.0, Total Affected=4.0, Total Affected Check=4.0, YEAR=None, MONTH=None, DAY_OF_WEEK=None, SEASON='Fall', PREV_DAY_INJURIES=0.0, NEXT_DAY_INJURIES=0.0, INJURY_CHANGE=2.0, BOROUGH_INJURY_RANK=1, 7DAY_AVG_INJURIES=1.2, CRITICAL_MISSING=False, ACCIDENT_SEVERITY='Medium', HAS_INJURIES=True, HAS_FATALITIES=False)]\n",
            "Successful Analysis:\n",
            "+-------+----------+--------------+--------+\n",
            "|BOROUGH|CRASH_DATE|DAILY_INJURIES|7DAY_AVG|\n",
            "+-------+----------+--------------+--------+\n",
            "+-------+----------+--------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "#initialize Spark with checkpoint configuration\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CrashAnalysis\") \\\n",
        "    .config(\"spark.executor.memory\", \"8g\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"400\") \\\n",
        "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#checkpoint directory (crucial for checkpointing)\n",
        "spark.sparkContext.setCheckpointDir('/tmp/checkpoints/')  # <-- ADD THIS LINE\n",
        "print(df.head(5))\n",
        "# Data preprocessing pipeline\n",
        "clean_df = (df\n",
        "    .filter(col(\"BOROUGH\").isNotNull())\n",
        "    .withColumn(\"CRASH_DATE\", to_date(col(\"CRASH DATE\"), \"yyyy-MM-dd\"))\n",
        "    .drop(\"CRASH DATE\")\n",
        "    .withColumn(\"YEAR\", year(col(\"CRASH_DATE\")))\n",
        "    .filter(col(\"YEAR\") >= 2018)\n",
        "    .repartition(400, \"BOROUGH\", \"YEAR\")\n",
        ")\n",
        "\n",
        "#range window specification\n",
        "range_window = (Window.partitionBy(\"BOROUGH\")\n",
        "                .orderBy(col(\"CRASH_DATE\").cast(\"long\"))\n",
        "                .rangeBetween(-6*86400, 0))\n",
        "\n",
        "#enhanced DataFrame with checkpointing\n",
        "enhanced_df = (clean_df\n",
        "    .withColumn(\"DAILY_INJURIES\", col(\"NUMBER OF PERSONS INJURED\"))\n",
        "    .withColumn(\"7DAY_AVG\", avg(col(\"DAILY_INJURIES\")).over(range_window))\n",
        ").checkpoint()  # Now works with the directory set\n",
        "\n",
        "try:\n",
        "    print(\"Successful Analysis:\")\n",
        "    enhanced_df.select(\n",
        "        \"BOROUGH\",\n",
        "        \"CRASH_DATE\",\n",
        "        \"DAILY_INJURIES\",\n",
        "        \"7DAY_AVG\"\n",
        "    ).limit(5).show(truncate=False)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Fallback to disk storage due to: {str(e)}\")\n",
        "    enhanced_df.write.parquet(\"./safe_results\")\n",
        "\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nt2i7Zp3LoS"
      },
      "outputs": [],
      "source": [
        "# ===============================================================================\n",
        "#SUMMARY STATISTICS AND FINAL VALIDATION\n",
        "# ===============================================================================\n",
        "# Purpose: Calculate summary statistics and perform final validation checks\n",
        "# Impact: Provides quality metrics and ensures data integrity before analytics\n",
        "# Demonstrates aggregation operations in a distributed environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CK9jB4zBY5-"
      },
      "outputs": [],
      "source": [
        "#summary statistics\n",
        "summary_stats = df.select(\n",
        "    F.count(\"*\").alias(\"TOTAL_RECORDS\"),\n",
        "    F.countDistinct(\"CRASH DATE\").alias(\"DISTINCT_DAYS\"),\n",
        "    F.countDistinct(\"BOROUGH\").alias(\"DISTINCT_BOROUGHS\"),\n",
        "    F.sum(\"Total Injuries\").alias(\"TOTAL_INJURIES\"),\n",
        "    F.sum(\"Total Deaths\").alias(\"TOTAL_DEATHS\"),\n",
        "    F.avg(\"NUMBER OF PERSONS INJURED\").alias(\"AVG_INJURIES_PER_ACCIDENT\"),\n",
        "    F.max(\"NUMBER OF PERSONS INJURED\").alias(\"MAX_INJURIES\"),\n",
        "    F.min(\"CRASH DATE\").alias(\"EARLIEST_DATE\"),\n",
        "    F.max(\"CRASH DATE\").alias(\"LATEST_DATE\")\n",
        ").collect()[0]\n",
        "\n",
        "#summary\n",
        "print(\"\\n===== DATA CLEANING AND PROCESSING SUMMARY =====\")\n",
        "print(f\"Total records processed: {summary_stats['TOTAL_RECORDS']}\")\n",
        "print(f\"Date range: {summary_stats['EARLIEST_DATE']} to {summary_stats['LATEST_DATE']}\")\n",
        "print(f\"Distinct days: {summary_stats['DISTINCT_DAYS']}\")\n",
        "print(f\"Distinct boroughs: {summary_stats['DISTINCT_BOROUGHS']}\")\n",
        "print(f\"Total injuries: {summary_stats['TOTAL_INJURIES']}\")\n",
        "print(f\"Total deaths: {summary_stats['TOTAL_DEATHS']}\")\n",
        "print(f\"Average injuries per accident: {summary_stats['AVG_INJURIES_PER_ACCIDENT']:.2f}\")\n",
        "print(f\"Maximum injuries in one accident: {summary_stats['MAX_INJURIES']}\")\n",
        "\n",
        "#cache the final cleaned dataframe for efficient reuse\n",
        "df.cache()\n",
        "\n",
        "print(\"\\nDistributed data cleaning and processing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vDPH0cdyi_6"
      },
      "source": [
        "# **Phase 3 - EDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaDi2SQNEkdB"
      },
      "outputs": [],
      "source": [
        "#initialized a new Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NYC Crash Data Analysis\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFpcdCVayi_7"
      },
      "source": [
        "**EDA1: Vehicles Contributing to accidents**\n",
        "\n",
        "The bar chart highlights the top 10 factors responsible for motor accidents.\n",
        "\n",
        "**Inference:**\n",
        "\n",
        "**Driver distraction**, **failure to yield** and **tailgating** are the major reasons for crashed.\n",
        "\n",
        "Furthermore, large number of **unspecified** accidents indicate missing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2phAtRlWp76c"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 1: Top Contributing Factors\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYJ09t48p-DO"
      },
      "outputs": [],
      "source": [
        "#occurrences of each contributing factor\n",
        "top_factors_df = df.groupBy('CONTRIBUTING FACTOR VEHICLE 1') \\\n",
        "                    .count() \\\n",
        "                    .orderBy(F.desc('count')) \\\n",
        "                    .limit(10)\n",
        "\n",
        "# Convert the Spark DataFrame to Pandas for easier plotting\n",
        "top_factors_pd = top_factors_df.toPandas()\n",
        "\n",
        "# Plot the top contributing factors\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='CONTRIBUTING FACTOR VEHICLE 1', y='count', data=top_factors_pd, palette=\"magma\")\n",
        "plt.title('Top 10 Contributing Factors to Crashes', fontsize=16)\n",
        "plt.xlabel('Contributing Factors', fontsize=14)\n",
        "plt.ylabel('Number of Crashes', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNq8sIrSyi_7"
      },
      "source": [
        "**EDA2: Vehicle Types Involved In Crashes**\n",
        "\n",
        "**Inference:**\n",
        "\n",
        "The most common vehicles involved in accidents are Sedans, SUVs and passenger vehicles.\n",
        "\n",
        "Taxis, tracks and buses contribute to accidents at a lower rate.\n",
        "\n",
        "The graph indicates that personal and commercial vehicles contribute differently to crash patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAFebaT8EvRA"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 2: Top Vehicle Types\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLLqHgQWyi_7"
      },
      "outputs": [],
      "source": [
        "top_vehicle_types_df = df.groupBy('VEHICLE TYPE CODE 1') \\\n",
        "                          .count() \\\n",
        "                          .orderBy(F.desc('count')) \\\n",
        "                          .limit(10)\n",
        "\n",
        "top_vehicle_types_pd = top_vehicle_types_df.toPandas()\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='VEHICLE TYPE CODE 1', y='count', data=top_vehicle_types_pd, palette=\"cividis\")\n",
        "plt.title('Top 10 Vehicle Types Involved in Crashes', fontsize=16)\n",
        "plt.xlabel('Vehicle Type', fontsize=14)\n",
        "plt.ylabel('Number of Crashes', fontsize=14)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc2tXHwtyi_8"
      },
      "source": [
        "**EDA3: Types of Injuries aggregrated based on Cyclists and Motorist**\n",
        "\n",
        "**Inference:**\n",
        "\n",
        "Motorist injuries dominates pedestarian/cyclist injuries.\n",
        "\n",
        "But pedastrians are more vulnerable as fatalities are more for pedastrians compared to motorists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnGnSfYrJk9p"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 3: Types of Injuries and Deaths\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS0XyKe3Lxvy"
      },
      "outputs": [],
      "source": [
        "#aggregating the data for Cyclist and Motorist injuries and deaths\n",
        "crash_types_df = df.select(\n",
        "    F.sum('NUMBER OF PEDESTRIANS INJURED').alias('Pedestrian Injuries'),\n",
        "    F.sum('NUMBER OF CYCLIST INJURED').alias('Cyclist Injuries'),\n",
        "    F.sum('NUMBER OF MOTORIST INJURED').alias('Motorist Injuries'),\n",
        "    F.sum('NUMBER OF PEDESTRIANS KILLED').alias('Pedestrian Deaths'),\n",
        "    F.sum('NUMBER OF CYCLIST KILLED').alias('Cyclist Deaths'),\n",
        "    F.sum('NUMBER OF MOTORIST KILLED').alias('Motorist Deaths')\n",
        ").collect()\n",
        "\n",
        "#convert the aggregated data to a dictionary\n",
        "crash_types_dict = {\n",
        "    'Pedestrian Injuries': crash_types_df[0]['Pedestrian Injuries'],\n",
        "    'Cyclist Injuries': crash_types_df[0]['Cyclist Injuries'],\n",
        "    'Motorist Injuries': crash_types_df[0]['Motorist Injuries'],\n",
        "    'Pedestrian Deaths': crash_types_df[0]['Pedestrian Deaths'],\n",
        "    'Cyclist Deaths': crash_types_df[0]['Cyclist Deaths'],\n",
        "    'Motorist Deaths': crash_types_df[0]['Motorist Deaths']\n",
        "}\n",
        "\n",
        "#convert to Pandas DataFrame for easier plotting\n",
        "crash_types_pd = pd.DataFrame(list(crash_types_dict.items()), columns=['Crash Type', 'Count'])\n",
        "\n",
        "#plotting the crash types and their frequencies\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Count', y='Crash Type', data=crash_types_pd, palette=\"mako\")\n",
        "plt.title('Types of Crashes and Their Frequencies')\n",
        "plt.xlabel('Count')\n",
        "plt.ylabel('Type of Crash')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjmF_IAs2OhG"
      },
      "source": [
        "**EDA4: Which hour sees most crashes in a day**\n",
        "\n",
        "**Inference:**\n",
        "\n",
        "Most crashes and accidents occur in between 8-10am in the morning (business and peak traffic hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKDSqls3KADv"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 4: Crashes by Hour of Day\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6hwq2Rkyi_8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "#convert CRASH TIME to a timestamp if necessary and create a new 'Hour' column\n",
        "df = df.withColumn(\"Hour\", F.hour(F.col(\"CRASH TIME\")))\n",
        "\n",
        "#group by the newly created 'Hour' column and count\n",
        "crashes_per_hour = df.groupBy('Hour').count()\n",
        "\n",
        "#calculate the average number of crashes per hour\n",
        "total_crashes = crashes_per_hour.agg(F.sum('count').alias('total_crashes')).collect()[0]['total_crashes']\n",
        "num_unique_hours = crashes_per_hour.count()\n",
        "average_crashes_per_hour = total_crashes / num_unique_hours\n",
        "\n",
        "#convert to Pandas DataFrame for plotting\n",
        "crashes_per_hour_pd = crashes_per_hour.toPandas()\n",
        "\n",
        "#plot the average number of crashes per hour\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Hour', y='count', data=crashes_per_hour_pd)\n",
        "plt.title('Average Number of Crashes per Hour of Day')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Crashes')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Average Number of Crashes per Hour: {average_crashes_per_hour}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSBiR6RO2bTS"
      },
      "source": [
        "**EDA5: Which Month is infamous for huge number of accidents**\n",
        "\n",
        "**Inference:**\n",
        "\n",
        "More accidents are seen in the months of July and December."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfICpXqFKKA6"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 5: Monthly Crash Trend\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZmqzSJgL8b8"
      },
      "outputs": [],
      "source": [
        "#Grouped by Year and Month to count the number of crashes per month\n",
        "monthly_crashes = df.groupBy('Year', 'Month').count()\n",
        "\n",
        "#Sorted first by Year and Month\n",
        "monthly_crashes = monthly_crashes.orderBy('Year', 'Month')\n",
        "\n",
        "#converted to Pandas DataFrame for plotting\n",
        "monthly_crashes_pd = monthly_crashes.toPandas()\n",
        "\n",
        "monthly_crashes_pd['Date'] = pd.to_datetime(monthly_crashes_pd[['Year', 'Month']].assign(DAY=1))\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(monthly_crashes_pd['Date'], monthly_crashes_pd['count'], marker='o', linestyle='-', color='b')\n",
        "plt.title('Number of Crashes per Month', fontsize=16)\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.ylabel('Number of Crashes', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2UNE_sFyi_9"
      },
      "source": [
        "**EDA6: Time Series Analysis to get more clarity of time of the day**\n",
        "\n",
        "**Inference:**\n",
        "\n",
        "The graph shows peak crash times:\n",
        "\n",
        "**8-10 AM:** Likely due to rush hour traffic\n",
        "\n",
        "**4-6 PM:**  Higher due to people commuting back home\n",
        "\n",
        "**6-11 PM:** Steady decrease in accidents due to reduced traffic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucXxLr2Oyi_9"
      },
      "source": [
        "**EDA6: Also, Crashes per month for last 12 years**\n",
        "\n",
        "**Inference:**\n",
        "\n",
        "The graph is relatively stable from 2012 to 2019 but we see a sudden drop in accidents from around early 2020. It is likey due to COVID-19 related lockdowns.\n",
        "\n",
        "Crashes increased again after pandemic but they are lower then pre pandemic levels.\n",
        "This suggests a potential long-term shift in traffic patterns due to COVID-19."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPuCD1jsqxKA"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 6: Time Series Decomposition\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KTz3JhSyi_9"
      },
      "outputs": [],
      "source": [
        "#time series decomposition - need to convert to pandas\n",
        "#daily crash counts\n",
        "daily_crashes_df = df.groupBy('CRASH DATE').count().orderBy('CRASH DATE')\n",
        "daily_crashes_pd = daily_crashes_df.toPandas()\n",
        "daily_crashes_pd.set_index('CRASH DATE', inplace=True)\n",
        "\n",
        "#the daily crashes time series\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(daily_crashes_pd.index, daily_crashes_pd['count'], label='Daily crashes', color='blue')\n",
        "plt.title('Daily Motor Vehicle Collisions in NYC')\n",
        "plt.xlabel('Date', fontsize=14)\n",
        "plt.ylabel('Number of Crashes', fontsize=14)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#Decompose the time series using pandas for statsmodels compatibility\n",
        "# Making sure the index is datetime and sorted\n",
        "daily_crashes_pd.index = pd.to_datetime(daily_crashes_pd.index)\n",
        "daily_crashes_pd = daily_crashes_pd.sort_index()\n",
        "\n",
        "#Handle missing dates by creating a full date range and joining\n",
        "full_date_range = pd.date_range(start=daily_crashes_pd.index.min(), end=daily_crashes_pd.index.max())\n",
        "daily_crashes_filled = daily_crashes_pd.reindex(full_date_range, fill_value=0)\n",
        "\n",
        "decomposition = seasonal_decompose(daily_crashes_filled['count'], model='additive', period=365)\n",
        "\n",
        "# Plot the decomposed components\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n",
        "decomposition.trend.plot(ax=ax1)\n",
        "ax1.set_title('Trend')\n",
        "decomposition.seasonal.plot(ax=ax2)\n",
        "ax2.set_title('Seasonality')\n",
        "decomposition.resid.plot(ax=ax3)\n",
        "ax3.set_title('Residuals')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNS43ywwyi_9"
      },
      "source": [
        "**EDA7: Distribution of Data by Boroughs**\n",
        "\n",
        "**Inference:**\n",
        "\n",
        "Brooklyn has the most number of crashes followed by queens and manhattan.\n",
        "\n",
        "Staten Island has the least crashes.\n",
        "\n",
        "This distribution actually follows the population density trends in NYC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDv9RkGkq9ko"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 7: Distribution by Borough\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUPRTJvpyi_-"
      },
      "outputs": [],
      "source": [
        "#get borough distribution\n",
        "borough_count_df = df.groupBy('BOROUGH').count().orderBy(F.desc('count'))\n",
        "borough_count_pd = borough_count_df.toPandas()\n",
        "\n",
        "#plotting the distribution of crashes by borough\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x=borough_count_pd['BOROUGH'], y=borough_count_pd['count'], palette=\"viridis\")\n",
        "plt.title('Distribution of Crashes by Borough', fontsize=16)\n",
        "plt.xlabel('Borough', fontsize=14)\n",
        "plt.ylabel('Number of Crashes', fontsize=14)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#create pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(borough_count_pd['count'], labels=borough_count_pd['BOROUGH'], autopct='%1.1f%%', colors=plt.cm.Paired.colors)\n",
        "plt.title('Borough-wise Crash Distribution', fontsize=16)\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_DujOykyi_-"
      },
      "source": [
        "**EDA8: Heatmaps**\n",
        "\n",
        "**Inference**\n",
        "\n",
        "There is a cluster of collisions in the vicinity of New York City, encompassing Manhattan and Brooklyn. There are also a few spots on the heatmap that show the severity of crashes close to the Manhattan Bridge, the far end of the Brooklyn-Queens Expressway, and Nostrand Avenue. There are several areas in East New York as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FgSs9iDrLoX"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 8: Heatmap of Crashes\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3dQdoMZrMdz"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "#For the heatmap, we need lat/long data from the Spark DataFrame\n",
        "df_geo_pd = df_geo.select('LATITUDE', 'LONGITUDE').toPandas()\n",
        "\n",
        "#base map\n",
        "m = folium.Map(location=[40.730610, -73.935242], zoom_start=10)  # Centered around NYC\n",
        "\n",
        "#main heatmap\n",
        "heat_data = [[row['LATITUDE'], row['LONGITUDE']] for _, row in df_geo_pd.iterrows()]\n",
        "HeatMap(heat_data, radius=8, max_zoom=13).add_to(m)\n",
        "\n",
        "#Save the map\n",
        "m.save(\"Heatmap.html\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FylTz_8Wyi_-"
      },
      "source": [
        "**EDA9: Residuals In Time Series (One more layer over normal Time Series plots) **\n",
        "\n",
        "**Inference**\n",
        "\n",
        "High Amount of Fluctuations are seen around the Covid-19 period which can be due to less cars and less civilians outside."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4iMksSuyi_-"
      },
      "outputs": [],
      "source": [
        "#EDA9: Residuals in time series\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "#total number of crashes per day, group by CRASH DATE\n",
        "daily_crashes = data.groupby('CRASH DATE').size()\n",
        "\n",
        "#plot style\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "#pltting the daily crashes time series\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(daily_crashes.index,daily_crashes.values, label='Daily crashes',color='blue')\n",
        "plt.title('Daily Motor Vehicle Collisions in NYC')\n",
        "plt.xlabel('Date', fontsize = 14)\n",
        "plt.ylabel('Number of Crashes', fontsize = 14)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "#decompose the time series\n",
        "decomposition = seasonal_decompose(daily_crashes, model='additive', period=365)\n",
        "\n",
        "#the decomposed components\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(15, 12))\n",
        "decomposition.trend.plot(ax=ax1)\n",
        "ax1.set_title('Trend')\n",
        "decomposition.seasonal.plot(ax=ax2)\n",
        "ax2.set_title('Seasonality')\n",
        "decomposition.resid.plot(ax=ax3)\n",
        "ax3.set_title('Residuals')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBY3i-PIyi_-"
      },
      "source": [
        "**EDA10: Severity plot on a web page (HTML) / Extension of EDA-8**\n",
        "\n",
        "**Inference**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdE7XhOsr1u2"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 10: Crash Severity Visualization\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeoLhUikyi__"
      },
      "outputs": [],
      "source": [
        "#Sample data for visualization (using pandas for folium)\n",
        "sample_data_severity = df_geo.select(\n",
        "    'LATITUDE', 'LONGITUDE', 'NUMBER OF PERSONS KILLED', 'NUMBER OF PERSONS INJURED'\n",
        ").sample(fraction=0.01, seed=42).toPandas()\n",
        "\n",
        "\n",
        "m_severity = folium.Map(location=[40.730610, -73.935242], zoom_start=10)\n",
        "\n",
        "for index, row in sample_data_severity.iterrows():\n",
        "    if row['NUMBER OF PERSONS KILLED'] > 0:\n",
        "        color = \"Black\"  # Fatalities\n",
        "        folium.features.RegularPolygonMarker(\n",
        "          location=[row['LATITUDE'], row['LONGITUDE']],\n",
        "          number_of_sides=3,\n",
        "          radius=5,\n",
        "          color=color,\n",
        "          fill=True,\n",
        "          fill_color=color\n",
        "        ).add_to(m_severity)\n",
        "    elif row['NUMBER OF PERSONS INJURED'] > 0:\n",
        "        color = \"Red\"  # Injuries\n",
        "        folium.CircleMarker(\n",
        "          location=[row['LATITUDE'], row['LONGITUDE']],\n",
        "          radius=5,\n",
        "          color=color,\n",
        "          fill=True,\n",
        "          fill_color=color\n",
        "        ).add_to(m_severity)\n",
        "    else:\n",
        "        color = \"Green\"  # No injuries or fatalities\n",
        "        folium.features.RegularPolygonMarker(\n",
        "          location=[row['LATITUDE'], row['LONGITUDE']],\n",
        "          number_of_sides=4,\n",
        "          radius=5,\n",
        "          color=color,\n",
        "          fill=True,\n",
        "          fill_color=color\n",
        "        ).add_to(m_severity)\n",
        "\n",
        "#Save the map\n",
        "m_severity.save(\"severity.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTsR2TgvsJw-"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 11: Seasonal Analysis of Injuries and Deaths\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR29hTpByi__"
      },
      "source": [
        "**EDA11: Season-wise Analysis of Total Injuries and Deaths**\n",
        "\n",
        "**Inference:**\n",
        "\n",
        "Fall and Summer have the highest number of affected indviduals.\n",
        "This could be due to the fact that more people are out in these seasons either on vacations, cruising the city for fun or doing outdoor activies.\n",
        "\n",
        "While spring and winter have less affected indviduals.\n",
        "This could be due to the fact that NYC gets chilly around this time and people usually don't step out much."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-m3qboZryi__"
      },
      "outputs": [],
      "source": [
        "#grouping by season for injuries\n",
        "season_injuries_df = df.groupBy('Season').agg(F.sum('Total Injuries').alias('Total Injuries'))\n",
        "season_injuries_pd = season_injuries_df.toPandas()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Season', y='Total Injuries', data=season_injuries_pd, palette='coolwarm')\n",
        "plt.title('Total Injuries by Season', fontsize=16)\n",
        "plt.xlabel('Season', fontsize=14)\n",
        "plt.ylabel('Total Injuries', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "season_deaths_df = df.groupBy('Season').agg(F.sum('Total Deaths').alias('Total Deaths'))\n",
        "season_deaths_pd = season_deaths_df.toPandas()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Season', y='Total Deaths', data=season_deaths_pd, palette='coolwarm')\n",
        "plt.title('Total Deaths by Season', fontsize=16)\n",
        "plt.xlabel('Season', fontsize=14)\n",
        "plt.ylabel('Total Deaths', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "season_affected_df = df.groupBy('Season').agg(F.sum('Total Affected').alias('Total Affected'))\n",
        "season_affected_pd = season_affected_df.toPandas()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Season', y='Total Affected', data=season_affected_pd, palette='coolwarm')\n",
        "plt.title('Total Affected by Season', fontsize=16)\n",
        "plt.xlabel('Season', fontsize=14)\n",
        "plt.ylabel('Total Affected', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB4KqGgFyi__"
      },
      "outputs": [],
      "source": [
        "#grouping the data by season and summing the total deaths\n",
        "season_injuries = data.groupby('Season')['Total Deaths'].sum().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Season', y='Total Deaths', data=season_injuries, palette='coolwarm')\n",
        "plt.title('Total Deaths by Season', fontsize=16)\n",
        "plt.xlabel('Season', fontsize=14)\n",
        "plt.ylabel('Total Deaths', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImQ98Fo1yi__"
      },
      "outputs": [],
      "source": [
        "#grouping the data by season and summing the total affected\n",
        "season_injuries = data.groupby('Season')['Total Affected'].sum().reset_index()\n",
        "\n",
        "#plotting the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Season', y='Total Affected', data=season_injuries, palette='coolwarm')\n",
        "plt.title('Total Affected by Season', fontsize=16)\n",
        "plt.xlabel('Season', fontsize=14)\n",
        "plt.ylabel('Total Affected', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlBK20j1yjAA"
      },
      "outputs": [],
      "source": [
        "#LET'S FIND ANSWERS TO:\n",
        "#What is the correlation between the time of day and the severity of crashes (fatalities, injuries) across different boroughs?\n",
        "#How do crash frequencies and causes vary in underserved communities compared to more affluent areas (based on zip codes)?\n",
        "#USING VISUALIZATIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8utEsUByjAA"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 12: Injuries by Hour\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_2aWMSvyjAA"
      },
      "outputs": [],
      "source": [
        "#group by hour and sum injuries\n",
        "hourly_injuries_df = df.groupBy('Hour').agg(F.sum('NUMBER OF PERSONS INJURED').alias('Total Injuries'))\n",
        "hourly_injuries_pd = hourly_injuries_df.toPandas()\n",
        "\n",
        "#line chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(hourly_injuries_pd['Hour'], hourly_injuries_pd['Total Injuries'], marker='o', color='orange')\n",
        "plt.title(\"Total Injuries by Hour of Day\")\n",
        "plt.xlabel(\"Hour of Day\")\n",
        "plt.ylabel(\"Number of Injuries\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0DE-br93PV7"
      },
      "outputs": [],
      "source": [
        "###############################################################################\n",
        "# VISUALIZATION 13: Map based analysis\n",
        "###############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPAyfzRtyjAA"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "zip_crash_data = data.groupby('ZIP CODE').size().reset_index(name='CRASH COUNT')\n",
        "\n",
        "#convert CRASH DATE and CRASH TIME to strings\n",
        "data['CRASH DATE'] = data['CRASH DATE'].astype(str)\n",
        "data['CRASH TIME'] = data['CRASH TIME'].astype(str)\n",
        "\n",
        "data[\"geometry\"] = data.apply(\n",
        "    lambda row: Point(row[\"LONGITUDE\"], row[\"LATITUDE\"]) if pd.notnull(row[\"LONGITUDE\"]) and pd.notnull(row[\"LATITUDE\"]) else None,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "#filter out rows with missing geometries\n",
        "data = data[data[\"geometry\"].notnull()]\n",
        "\n",
        "gdf = gpd.GeoDataFrame(data, geometry=\"geometry\")\n",
        "\n",
        "#set the coordinate reference system (CRS) to WGS84 (EPSG:4326)\n",
        "gdf.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "#ensure ZIP CODE is a string\n",
        "gdf['ZIP CODE'] = gdf['ZIP CODE'].astype(str)\n",
        "\n",
        "#merge with geo data\n",
        "merged = gdf.merge(zip_crash_data, on='ZIP CODE')\n",
        "\n",
        "#create map\n",
        "m = folium.Map(location=[40.7128, -74.0060], zoom_start=10)\n",
        "folium.Choropleth(\n",
        "    geo_data=merged,\n",
        "    name=\"choropleth\",\n",
        "    data=merged,\n",
        "    columns=[\"ZIP CODE\", \"CRASH COUNT\"],\n",
        "    key_on=\"feature.properties.ZIP CODE\",\n",
        "    fill_color=\"YlOrRd\",\n",
        "    fill_opacity=0.7,\n",
        "    line_opacity=0.2,\n",
        ").add_to(m)\n",
        "\n",
        "m.save(\"crash_map.html\")\n",
        "m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDbApY9BsbEc"
      },
      "outputs": [],
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqvBzn_bHGIK"
      },
      "outputs": [],
      "source": [
        "# Install ngrok\n",
        "!pip install pyngrok\n",
        "!pip install graphviz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmXU_VvvozNC"
      },
      "source": [
        "# **Phase 3 - ML Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hKDV-tmODqF"
      },
      "outputs": [],
      "source": [
        "###################################\n",
        "# MULTIPLE SPARK ML MODELS\n",
        "# Using MinMaxScaler + VectorClipper for Naive Bayes to ensure nonnegative features.\n",
        "###################################\n",
        "import pyspark\n",
        "from graphviz import Digraph\n",
        "import re\n",
        "from IPython.display import display\n",
        "# from pyngrok import ngrok\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, when, to_timestamp, dayofweek, hour, trim,\n",
        "    row_number\n",
        ")\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "#spark ML libraries\n",
        "from pyspark.ml.feature import (\n",
        "    StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, MinMaxScaler\n",
        ")\n",
        "from pyspark.ml.classification import (\n",
        "    LogisticRegression,\n",
        "    DecisionTreeClassifier,\n",
        "    GBTClassifier,\n",
        "    RandomForestClassifier,\n",
        "    MultilayerPerceptronClassifier,\n",
        "    LinearSVC,\n",
        "    NaiveBayes\n",
        ")\n",
        "from pyspark.ml.clustering import KMeans as SparkKMeans\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import (\n",
        "    BinaryClassificationEvaluator,\n",
        "    MulticlassClassificationEvaluator,\n",
        "    ClusteringEvaluator\n",
        ")\n",
        "\n",
        "#python visualization & utilities\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "##########################################\n",
        "# CUSTOM TRANSFORMER: VectorClipper\n",
        "# This transformer clips any negative values in a vector column to zero.\n",
        "##########################################\n",
        "from pyspark.ml import Transformer\n",
        "from pyspark import keyword_only\n",
        "from pyspark.ml.linalg import VectorUDT, Vectors\n",
        "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
        "from pyspark.sql.functions import udf\n",
        "class VectorClipper(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol=\"scaled_features\", outputCol=\"scaled_features\"):\n",
        "        super(VectorClipper, self).__init__()\n",
        "        self.inputCol = inputCol\n",
        "        self.outputCol = outputCol\n",
        "\n",
        "    def _transform(self, dataset):\n",
        "        def clip_vector(v):\n",
        "            # Convert vector to array, clip negatives to 0, then back to vector.\n",
        "            arr = v.toArray() if hasattr(v, \"toArray\") else v\n",
        "            clipped = [max(0.0, x) for x in arr]\n",
        "            return Vectors.dense(clipped)\n",
        "        clip_udf = udf(clip_vector, VectorUDT())\n",
        "        return dataset.withColumn(self.outputCol, clip_udf(dataset[self.inputCol]))\n",
        "\n",
        "#These functions will visualize the DAG diagrams within the notebook\n",
        "def visualize_spark_pipeline(stages, title=\"Spark ML Pipeline DAG\"):\n",
        "    dot = Digraph(format='png')\n",
        "    dot.attr(rankdir='LR', fontsize='12', compound='true')\n",
        "\n",
        "    with dot.subgraph(name='cluster_pipeline') as c:\n",
        "        c.attr(label=title, style='dashed')\n",
        "        for i, stage in enumerate(stages):\n",
        "            stage_name = stage.__class__.__name__\n",
        "            node_id = f\"stage_{i}\"\n",
        "            c.node(node_id, stage_name, shape=\"box\")\n",
        "            if i > 0:\n",
        "                c.edge(f\"stage_{i-1}\", node_id)\n",
        "    display(dot)\n",
        "\n",
        "def visualize_operations_dag(plan_str, title=\"Spark Physical Plan DAG\"):\n",
        "    from graphviz import Digraph\n",
        "    from IPython.display import display\n",
        "    import re\n",
        "\n",
        "    dot = Digraph(format='png')\n",
        "    dot.attr(rankdir='TB', fontsize='10')\n",
        "\n",
        "    lines = plan_str.strip().split('\\n')\n",
        "    nodes, edges, stack = [], [], []\n",
        "\n",
        "    for line in lines:\n",
        "        indent = len(line) - len(line.lstrip())\n",
        "        raw_label = line.strip()\n",
        "\n",
        "        op_match = re.search(r'[\\*\\+\\-:]*\\(?\\d*\\)?\\s*([A-Za-z]+)', raw_label)\n",
        "        op_name = op_match.group(1).strip() if op_match else raw_label\n",
        "        label = op_name\n",
        "\n",
        "        node_id = f\"node_{len(nodes)}\"\n",
        "        nodes.append((node_id, label))\n",
        "\n",
        "        while stack and stack[-1][1] >= indent:\n",
        "            stack.pop()\n",
        "        if stack:\n",
        "            edges.append((stack[-1][0], node_id))\n",
        "\n",
        "        stack.append((node_id, indent))\n",
        "\n",
        "    for node_id, label in nodes:\n",
        "        dot.node(node_id, label, shape=\"box\", style=\"filled\", fillcolor=\"lightblue\")\n",
        "\n",
        "    for src, dst in edges:\n",
        "        dot.edge(src, dst)\n",
        "\n",
        "    dot.attr(label=title)\n",
        "    display(dot)\n",
        "\n",
        "def visualize_stagewise_rdd_dag(debug_string, title=\"RDD Stagewise DAG\"):\n",
        "\n",
        "    if isinstance(debug_string, bytes):\n",
        "        debug_string = debug_string.decode(\"utf-8\")\n",
        "\n",
        "    lines = debug_string.strip().splitlines()\n",
        "\n",
        "    wide_deps = [\n",
        "        \"ShuffledRDD\", \"ShuffleDependency\",\n",
        "        \"reduceByKey\", \"groupByKey\", \"sortByKey\",\n",
        "        \"repartition\", \"coalesce\", \"join\", \"cogroup\"\n",
        "    ]\n",
        "\n",
        "    dot = Digraph(format='png')\n",
        "    dot.attr(rankdir='TB', fontsize='10')\n",
        "\n",
        "    stages = {0: []}\n",
        "    edges = []\n",
        "    current_stage = 0\n",
        "    last_node = None\n",
        "    carry_edge = None\n",
        "\n",
        "    for raw in lines:\n",
        "        line = raw.strip()\n",
        "\n",
        "        m = re.match(r'^\\(?\\d+\\)?\\s*([A-Za-z]+)', line)\n",
        "        if m:\n",
        "            op = m.group(1)\n",
        "            op = re.sub(r'RDD$', '', op)\n",
        "            label = op[0].lower() + op[1:]\n",
        "        else:\n",
        "            label = line\n",
        "\n",
        "        node_id = f\"n{current_stage}_{len(stages[current_stage])}\"\n",
        "        stages[current_stage].append((node_id, label))\n",
        "\n",
        "        if carry_edge:\n",
        "            edges.append((carry_edge, node_id))\n",
        "            carry_edge = None\n",
        "        elif last_node:\n",
        "            edges.append((last_node, node_id))\n",
        "\n",
        "        last_node = node_id\n",
        "\n",
        "        if any(dep in line for dep in wide_deps):\n",
        "            carry_edge = last_node\n",
        "            current_stage += 1\n",
        "            stages[current_stage] = []\n",
        "            last_node = None\n",
        "\n",
        "    for sid, nodes in stages.items():\n",
        "        with dot.subgraph(name=f\"cluster_{sid}\") as c:\n",
        "            c.attr(label=f\"Stage {sid}\", color=\"red\", style=\"dashed\")\n",
        "            for nid, lbl in nodes:\n",
        "                c.node(nid, lbl, shape=\"box\", style=\"filled\", fillcolor=\"lightblue\")\n",
        "\n",
        "    for src, dst in edges:\n",
        "        dot.edge(src, dst)\n",
        "\n",
        "    dot.attr(label=title)\n",
        "    display(dot)\n",
        "\n",
        "\n",
        "\n",
        "#created a new spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Phase3_MultipleModels_Extended\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#Uncomment this if you need to access Spark UI in colab\n",
        "# ngrok.set_auth_token(\"2veWgSMKSZUXoq3cDHk6gj7KPy2_2Soz335NRQEtrGw9tLcwC\")\n",
        "# # Start ngrok tunnel\n",
        "# http_tunnel = ngrok.connect(4040)\n",
        "# public_url = http_tunnel.public_url\n",
        "# print(f\"Spark UI: {public_url}\")\n",
        "\n",
        "#data loading\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df_spark = spark.read.csv(\n",
        "    \"/content/drive/MyDrive/DIC Project/cleaned_dataset.csv\",\n",
        "    header=True,\n",
        "    inferSchema=True\n",
        ")\n",
        "\n",
        "#optional sampling if needed\n",
        "df_spark = df_spark.sample(False, 0.25, seed=42).sample(False, 0.25, seed=42)\n",
        "\n",
        "#feature engineering using RDD\n",
        "df_spark = df_spark.withColumn(\"CRASH DATE\", to_timestamp(col(\"CRASH DATE\"), \"yyyy-MM-dd\"))\n",
        "df_spark = df_spark.withColumn(\"DayOfWeek\", dayofweek(col(\"CRASH DATE\")) - 2)\n",
        "df_spark = df_spark.withColumn(\"CRASH TIME\", to_timestamp(col(\"CRASH TIME\"), \"H:mm\"))\n",
        "df_spark = df_spark.withColumn(\"HourOfDay\", hour(col(\"CRASH TIME\")))\n",
        "df_spark = df_spark.withColumn(\"BOROUGH\", trim(col(\"BOROUGH\")))\n",
        "df_spark = df_spark.withColumn(\"ZIP CODE\", col(\"ZIP CODE\").cast(FloatType()))\n",
        "\n",
        "for factor_col in [\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 1\",\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 2\",\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 3\"]:\n",
        "    df_spark = df_spark.fillna(\"Unknown\", subset=[factor_col])\n",
        "\n",
        "# Windowing: accident counts per borough\n",
        "windowSpec = Window.partitionBy(\"BOROUGH\").orderBy(\"CRASH DATE\")\n",
        "df_spark = df_spark.withColumn(\"Borough_RowNum\", row_number().over(windowSpec))\n",
        "\n",
        "# RDD: count total accidents per borough and rejoin\n",
        "rdd_borough = (\n",
        "    df_spark.rdd\n",
        "    .map(lambda row: (row[\"BOROUGH\"], 1))\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "    .repartition(4)\n",
        ")\n",
        "\n",
        "rdd_borough.count()\n",
        "debug_str = rdd_borough.toDebugString()\n",
        "visualize_stagewise_rdd_dag(debug_str, title=\"Stagewise RDD DAG – Borough Accident Aggregation\")\n",
        "\n",
        "\n",
        "df_borough_count = rdd_borough.toDF([\"BOROUGH\", \"Total_Accidents\"])\n",
        "df_spark = df_spark.join(df_borough_count, on=\"BOROUGH\", how=\"left\")\n",
        "\n",
        "#create binary target: SEVERE_ACCIDENT if any injury or fatality occurred\n",
        "df_spark = df_spark.withColumn(\"SEVERE_ACCIDENT\", when(\n",
        "    (col(\"NUMBER OF PERSONS INJURED\") > 0) | (col(\"NUMBER OF PERSONS KILLED\") > 0), 1).otherwise(0))\n",
        "\n",
        "#rename HourOfDay to CRASH_HOUR for clarity\n",
        "df_spark = df_spark.withColumnRenamed(\"HourOfDay\", \"CRASH_HOUR\")\n",
        "\n",
        "#feature and label selection\n",
        "feature_cols = [\n",
        "    \"CONTRIBUTING FACTOR VEHICLE 1\",\n",
        "    \"BOROUGH\",\n",
        "    \"CRASH_HOUR\",\n",
        "    \"VEHICLE TYPE CODE 1\",\n",
        "    \"LATITUDE\",\n",
        "    \"LONGITUDE\",\n",
        "    \"Borough_RowNum\",\n",
        "    \"Total_Accidents\"\n",
        "]\n",
        "label_col = \"SEVERE_ACCIDENT\"\n",
        "\n",
        "#drop rows with nulls in features or label\n",
        "df_spark = df_spark.dropna(subset=feature_cols + [label_col])\n",
        "\n",
        "print('######## DAG FOR PROCESSING #########')\n",
        "df_spark.explain(False)\n",
        "\n",
        "#splitting the data\n",
        "train_df, test_df = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "#common Pipeline Stages\n",
        "borough_indexer = StringIndexer(inputCol=\"BOROUGH\", outputCol=\"BOROUGH_idx\", handleInvalid=\"keep\")\n",
        "factor_indexer = StringIndexer(inputCol=\"CONTRIBUTING FACTOR VEHICLE 1\", outputCol=\"FACTOR1_idx\", handleInvalid=\"keep\")\n",
        "vehicle_indexer = StringIndexer(inputCol=\"VEHICLE TYPE CODE 1\", outputCol=\"VEHICLE_idx\", handleInvalid=\"keep\")\n",
        "\n",
        "encoder = OneHotEncoder(\n",
        "    inputCols=[\"BOROUGH_idx\", \"FACTOR1_idx\", \"VEHICLE_idx\"],\n",
        "    outputCols=[\"BOROUGH_enc\", \"FACTOR1_enc\", \"VEHICLE_enc\"]\n",
        ")\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"CRASH_HOUR\", \"LATITUDE\", \"LONGITUDE\", \"Borough_RowNum\", \"Total_Accidents\",\n",
        "               \"BOROUGH_enc\", \"FACTOR1_enc\", \"VEHICLE_enc\"],\n",
        "    outputCol=\"assembled_features\"\n",
        ")\n",
        "\n",
        "#StandardScaler used for all models except Naive Bayes (which requires nonnegative features)\n",
        "scaler = StandardScaler(inputCol=\"assembled_features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
        "\n",
        "#MinMaxScaler scales features to [0, 1]\n",
        "minmax_scaler = MinMaxScaler(inputCol=\"assembled_features\", outputCol=\"scaled_features\")\n",
        "\n",
        "#VectorClipper will clip any residual negative values to 0\n",
        "clipper = VectorClipper(inputCol=\"scaled_features\", outputCol=\"scaled_features\")\n",
        "\n",
        "##########################################\n",
        "# SUPERVISED MODELS USING SPARK ML\n",
        "##########################################\n",
        "mlp_layers = [250, 32, 16, 2]\n",
        "\n",
        "#models dictionary.\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(featuresCol=\"scaled_features\", labelCol=label_col, maxIter=100),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(featuresCol=\"scaled_features\", labelCol=label_col, maxDepth=5),\n",
        "    \"GBT\": GBTClassifier(featuresCol=\"scaled_features\", labelCol=label_col, maxIter=50, maxDepth=5),\n",
        "    \"Random Forest\": RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=label_col, numTrees=50),\n",
        "    \"Multilayer Perceptron\": MultilayerPerceptronClassifier(featuresCol=\"scaled_features\", labelCol=label_col,\n",
        "                                                            maxIter=100, layers=mlp_layers),\n",
        "    \"Linear SVC\": LinearSVC(featuresCol=\"scaled_features\", labelCol=label_col, maxIter=100),\n",
        "    \"Naive Bayes\": NaiveBayes(featuresCol=\"scaled_features\", labelCol=label_col)\n",
        "}\n",
        "\n",
        "#evaluators\n",
        "evaluator_auc = BinaryClassificationEvaluator(labelCol=label_col, rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=label_col, metricName=\"accuracy\")\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=label_col, metricName=\"f1\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n=== Model: {name} ===\")\n",
        "    pipeline_stages = [borough_indexer, factor_indexer, vehicle_indexer, encoder, assembler]\n",
        "    if name == \"Naive Bayes\":\n",
        "        pipeline_stages += [minmax_scaler, clipper, model]\n",
        "    else:\n",
        "        pipeline_stages += [scaler, model]\n",
        "\n",
        "    pipeline = Pipeline(stages=pipeline_stages)\n",
        "\n",
        "    visualize_spark_pipeline(pipeline_stages, title=f\"{name} - ML Pipeline DAG\")\n",
        "\n",
        "    fitted_model = pipeline.fit(train_df)\n",
        "    preds = fitted_model.transform(test_df)\n",
        "\n",
        "    plan = preds._jdf.queryExecution().executedPlan().toString()\n",
        "    visualize_operations_dag(plan, title=f\"{name} - Physical Execution DAG\")\n",
        "\n",
        "    auc = evaluator_auc.evaluate(preds)\n",
        "    acc = evaluator_acc.evaluate(preds)\n",
        "    f1 = evaluator_f1.evaluate(preds)\n",
        "\n",
        "    print(f\"AUC: {auc:.4f}, Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
        "\n",
        "    cm = preds.groupBy(label_col, \"prediction\").count().toPandas()\n",
        "    cm_pivot = cm.pivot(index=label_col, columns=\"prediction\", values=\"count\").fillna(0).values\n",
        "\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(cm_pivot, annot=True, fmt=\".0f\", cmap=\"Blues\")\n",
        "    plt.title(f\"{name} Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n",
        "\n",
        "##########################################\n",
        "# UNSUPERVISED CLUSTERING: KMEANS (Spark ML)\n",
        "##########################################\n",
        "# Apply full transformation to the complete dataset to obtain scaled features.\n",
        "pipeline_full = Pipeline(stages=[borough_indexer, factor_indexer, vehicle_indexer, encoder, assembler, scaler])\n",
        "fitted_pipeline_full = pipeline_full.fit(df_spark)\n",
        "df_transformed = fitted_pipeline_full.transform(df_spark)\n",
        "\n",
        "visualize_spark_pipeline(pipeline_full.getStages(), title=\"KMeans - ML Feature Engineering DAG\")\n",
        "\n",
        "kmeans_plan = df_transformed._jdf.queryExecution().executedPlan().toString()\n",
        "visualize_operations_dag(kmeans_plan, title=\"KMeans - Physical Execution DAG\")\n",
        "\n",
        "k_range = list(range(2, 10))\n",
        "inertia_list = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = SparkKMeans(featuresCol=\"scaled_features\", k=k, seed=42)\n",
        "    kmodel = kmeans.fit(df_transformed)\n",
        "    inertia = kmodel.summary.trainingCost  # Sum of squared distances\n",
        "    inertia_list.append(inertia)\n",
        "\n",
        "    evaluator_cluster = ClusteringEvaluator(featuresCol=\"scaled_features\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
        "    silhouette = evaluator_cluster.evaluate(kmodel.transform(df_transformed))\n",
        "    silhouette_scores.append(silhouette)\n",
        "    print(f\"k={k}: Inertia={inertia:.2f}, Silhouette Score={silhouette:.3f}\")\n",
        "\n",
        "#plot the elbow graph (Inertia vs. k)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(k_range, inertia_list, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k (Spark KMeans)')\n",
        "plt.show()\n",
        "\n",
        "optimal_k = k_range[np.argmax(silhouette_scores)]\n",
        "print(f\"Optimal k based on silhouette score: {optimal_k}\")\n",
        "\n",
        "kmeans_optimal = SparkKMeans(featuresCol=\"scaled_features\", k=optimal_k, seed=42)\n",
        "kmodel_opt = kmeans_optimal.fit(df_transformed)\n",
        "df_clustered = kmodel_opt.transform(df_transformed)\n",
        "\n",
        "from pyspark.ml.feature import PCA\n",
        "pca = PCA(k=2, inputCol=\"scaled_features\", outputCol=\"pca_features\")\n",
        "pca_model = pca.fit(df_clustered)\n",
        "df_pca = pca_model.transform(df_clustered)\n",
        "\n",
        "pdf_pca = df_pca.select(\"pca_features\").toPandas()\n",
        "pdf_clusters = df_pca.select(\"prediction\").toPandas()\n",
        "pca_array = np.array([row[0] for row in pdf_pca.values])\n",
        "df_plot = pd.DataFrame(pca_array, columns=['PC1', 'PC2'])\n",
        "df_plot['Cluster'] = pdf_clusters\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=\"PC1\", y=\"PC2\", hue=\"Cluster\", data=df_plot, palette=\"deep\")\n",
        "plt.title(\"KMeans Clusters\")\n",
        "plt.show()\n",
        "\n",
        "#stop spark session\n",
        "spark.stop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
